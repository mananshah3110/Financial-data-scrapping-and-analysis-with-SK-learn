{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Stock data scraping and analysis with sk-learn"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- __version__ = 2.00\n",
        "- __author__ = Manan Shah"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load libraries"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import numpy as np\n",
        "import time\n",
        "from datetime import date, datetime, timedelta\n",
        "\n",
        "import investpy\n",
        "\n",
        "# Plotting library\n",
        "import matplotlib as mpl\n",
        "from matplotlib import dates as mdates\n",
        "from matplotlib import pyplot as plt\n",
        "from matplotlib import style\n",
        "\n",
        "years = mdates.YearLocator()\n",
        "months = mdates.MonthLocator()\n",
        "date_fmt = mdates.DateFormatter(\"%b-%y\")  # Set mon-year format\n",
        "\n",
        "from warnings import warn\n",
        "\n",
        "from IPython.display import Markdown as MD\n",
        "from IPython.display import display\n",
        "\n",
        "# lxml is much much faster than requsts_html at least for a single call. I sill have to check for simultaneous multiple calls.\n",
        "from lxml import etree\n",
        "from requests import Request, Session  # for http requests\n",
        "\n",
        "import pandas as pd\n",
        "from pandas import DataFrame as DF\n",
        "from pandas.plotting import scatter_matrix\n",
        "\n",
        "pd.set_option(\n",
        "    \"display.column_space\",\n",
        "    12,\n",
        "    \"display.max_colwidth\",\n",
        "    12,\n",
        "    \"display.max_rows\",\n",
        "    10,\n",
        "    \"display.colheader_justify\",\n",
        "    \"center\",\n",
        "    \"display.date_dayfirst\",\n",
        "    True,\n",
        "    \"display.max_columns\",\n",
        "    15,\n",
        "    \"float_format\",\n",
        "    \"{:.2f}\".format,\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-07-16T15:35:30.379103Z",
          "iopub.status.busy": "2021-07-16T15:35:30.378106Z",
          "iopub.status.idle": "2021-07-16T15:35:30.402041Z",
          "shell.execute_reply": "2021-07-16T15:35:30.401045Z",
          "shell.execute_reply.started": "2021-07-16T15:35:30.379103Z"
        },
        "tags": []
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## User defined functions\n",
        "### Format display text"
      ],
      "metadata": {
        "execution": {
          "iopub.execute_input": "2020-08-23T06:48:19.970107Z",
          "iopub.status.busy": "2020-08-23T06:48:19.970107Z",
          "iopub.status.idle": "2020-08-23T06:48:19.984071Z",
          "shell.execute_reply": "2020-08-23T06:48:19.983071Z",
          "shell.execute_reply.started": "2020-08-23T06:48:19.970107Z"
        },
        "tags": []
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def printmd(string, color=None):\n",
        "    \"\"\"\n",
        "    Displays the string in color with markdown effects.\n",
        "\n",
        "    Parameters:\n",
        "        string (str): \"The string to be printed.\"\n",
        "        color (str): \"Color of the string (e.g., \"green\", \"red\").\"\n",
        "\n",
        "    Display:\n",
        "        display(MD(colorstr)): A colored string with markdown effects.\n",
        "\n",
        "    Example: printmd(f\"**the value: {a}**\", color = \"green\")\n",
        "    \"\"\"\n",
        "\n",
        "    colorstr = \"<span style='color:{}'>{}</span>\".format(\n",
        "        color,\n",
        "        string,\n",
        "    )\n",
        "    display(MD(colorstr))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-07-16T15:35:35.592483Z",
          "iopub.status.busy": "2021-07-16T15:35:35.591484Z",
          "iopub.status.idle": "2021-07-16T15:35:35.606444Z",
          "shell.execute_reply": "2021-07-16T15:35:35.605448Z",
          "shell.execute_reply.started": "2021-07-16T15:35:35.592483Z"
        },
        "tags": []
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ä## Element tree with requests and lxml"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "def request_lxml_etree(url: str) -> \"element tree bytecode\":\n",
        "    \"\"\"\n",
        "    Get content of the server's response using requests and lxml. The function returns element-tree for further desired element selection if the rquest is \n",
        "    successfull, otherwise returns the status code and the respective error message.\n",
        "\n",
        "    Parameters:\n",
        "        url (str): Target URL from which the desired data is to be scraped.\n",
        "\n",
        "    Returns:\n",
        "        element_tree (etree object): HTML/XML element object.\n",
        "\n",
        "    Example:\n",
        "        tree = request_lxml_etree(url=URL)\n",
        "    \"\"\"\n",
        "\n",
        "    # Standard variables\n",
        "    headers = {\n",
        "        \"user-agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.88 Safari/537.36\"\n",
        "    }\n",
        "    timeout = (5, 10)\n",
        "\n",
        "    # Response object containing a server’s response to an HTTP request.\n",
        "    # Session object allows making several requests to the same host, the underlying TCP connection will be reused, which can result in a significant performance increase.\n",
        "    response = Session().get(url=url, headers=headers, timeout=timeout, verify=True)\n",
        "\n",
        "    # the action requested by the client was received, understood, and accepted.\n",
        "    if response.status_code == 200:\n",
        "        # Read the raw bytes of the server’s response content.\n",
        "        byte_code = response.content  # content as a normal UTF-8 encoded Python string.\n",
        "\n",
        "        # lxml.etree offers a lot more functionality, such as XPath, XSLT, Relax NG, and XML Schema support.\n",
        "        elelment_tree = etree.HTML(byte_code)\n",
        "\n",
        "        return elelment_tree\n",
        "\n",
        "    # client must take additional action to complete the request.\n",
        "    elif response.status_code == 301:\n",
        "        printmd(\"Status code:\", response.status_code)\n",
        "        warn(\n",
        "            f\"The {URL} is moved permanently and might require a different 'GET' method.\"\n",
        "        )\n",
        "\n",
        "    elif response.status_code == 307:\n",
        "        printmd(\"Status code:\", response.status_code)\n",
        "        warn(\n",
        "            f\"The {URL} is redirected temporarily. The future requests should still use the original URL and 'GET' method.\"\n",
        "        )\n",
        "\n",
        "    elif response.status_code == 308:\n",
        "        printmd(\"Status code:\", response.status_code)\n",
        "        warn(\n",
        "            f\"The {URL} is moved permanently and will not allow a different 'GET' method.\"\n",
        "        )\n",
        "\n",
        "    # the error seems to have been caused by the client\n",
        "    elif response.status_code == 400:\n",
        "        printmd(\"Status code:\", response.status_code)\n",
        "        raise Exception(\n",
        "            \"Bad request! The server cannot or will not process the request due to an apparent client error (e.g., malformed request syntax, size too large, invalid request message framing, or deceptive request routing)\"\n",
        "        )\n",
        "\n",
        "    elif response.status_code == 401:\n",
        "        printmd(\"Status code:\", response.status_code)\n",
        "        raise Exception(\n",
        "            \"Unauthorized request! An authentication is required and has failed or has not yet been provided. The response must include a authentication data in the header. Check the header dict.\"\n",
        "        )\n",
        "\n",
        "    elif response.status_code == 403:\n",
        "        printmd(\"Status code:\", response.status_code)\n",
        "        raise Exception(\n",
        "            \"Request is forbidden! The request contained valid data and was understood by the server, but the server is refusing action (to avoid scraping).\"\n",
        "        )\n",
        "\n",
        "    elif response.status_code == 404:\n",
        "        printmd(\"Status code:\", response.status_code)\n",
        "        raise Exception(\"The requested resource could not be found.\")\n",
        "\n",
        "    elif response.status_code == 408:\n",
        "        printmd(\"Status code:\", response.status_code)\n",
        "        raise Exception(\"Request Timeout. Check the timeout tupple.\")\n",
        "\n",
        "    elif response.status_code == 423:\n",
        "        printmd(\"Status code:\", response.status_code)\n",
        "        raise Exception(\"The resource that is being accessed is locked.\")\n",
        "\n",
        "    elif response.status_code == 429:\n",
        "        printmd(\"Status code:\", response.status_code)\n",
        "        raise Exception(\"Too many requests in a given amount of time.\")\n",
        "\n",
        "    #  the server is aware that it has encountered an error or is otherwise incapable of performing the request.\n",
        "    elif response.status_code == 503:\n",
        "        printmd(\"Status code:\", response.status_code)\n",
        "        raise Exception(\n",
        "            \"The server cannot handle the request (because it is overloaded or down for maintenance). Generally, this is a temporary state.\"\n",
        "        )\n",
        "\n",
        "    else:\n",
        "        printmd(\"Status code:\", response.status_code)\n",
        "        raise Exception(\"Something went wrong!\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-07-16T15:35:45.365487Z",
          "iopub.status.busy": "2021-07-16T15:35:45.364491Z",
          "iopub.status.idle": "2021-07-16T15:35:45.394410Z",
          "shell.execute_reply": "2021-07-16T15:35:45.393443Z",
          "shell.execute_reply.started": "2021-07-16T15:35:45.365487Z"
        },
        "tags": []
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Table element byte code from a webpage"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "def etree_table(\n",
        "    etree: \"element tree bytecode\", xpath_table: str\n",
        ") -> \"table element bytecode\":\n",
        "    \"\"\"\n",
        "    Returns the desired table element using the table_element_xpath.\n",
        "\n",
        "    Parameters:\n",
        "        etree (element object): The byte code of selected element tree.\n",
        "        xpath_table (str): XPath to the desired table to be scraped.\n",
        "\n",
        "    Returns:\n",
        "        table_element (element object): The byte code of selected table element.\n",
        "\n",
        "    Example:\n",
        "        table = etree_table(etree=stock_tree, xpath_table = './/span[contains(@class, \"nextpag\")]')\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # The desired (here single table) elements are fetched as a list from the element tree.\n",
        "        table_element_list = etree.xpath(xpath_table)\n",
        "\n",
        "        if len(table_element_list) != 0:\n",
        "            table_element = table_element_list[0]\n",
        "            return table_element\n",
        "\n",
        "        else:\n",
        "            raise Exception(\n",
        "                f\"No table found. Check the XPath query syntex in {xpath_table}.\"\n",
        "            )\n",
        "\n",
        "    except:\n",
        "        raise Exception(\n",
        "            f\"Desired class in the XPath is not found! Check the XPath/class in {xpath_table}.\"\n",
        "        )"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-07-16T15:35:48.076476Z",
          "iopub.status.busy": "2021-07-16T15:35:48.076476Z",
          "iopub.status.idle": "2021-07-16T15:35:48.094429Z",
          "shell.execute_reply": "2021-07-16T15:35:48.093431Z",
          "shell.execute_reply.started": "2021-07-16T15:35:48.076476Z"
        },
        "tags": []
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pagination of webpage (next URL)"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "def etree_URLnext(\n",
        "    etree: \"element tree bytecode\", xpath_URLnext: str\n",
        ") -> \"pagination URL link (str)\":\n",
        "    \"\"\"\n",
        "    Returns the next pagination link if available using the nextpage_link_xpath otherwise raises an error.\n",
        "\n",
        "    Parameters:\n",
        "        etree (element object): The byte code of selected element tree.\n",
        "        xpath_URLnext (str): XPath for \"next page\" button.\n",
        "\n",
        "    Returns:\n",
        "        URLnext (str): Next page URL until it reached the last paging.\n",
        "\n",
        "    Example:\n",
        "        url_next = tree_nextpage_link(element_tree=tree, xpath_nextpage= xpath_URLnext)\n",
        "    \"\"\"\n",
        "    try:\n",
        "        element = etree.xpath(xpath_URLnext)[0]\n",
        "\n",
        "        if element is not None:\n",
        "            URLnext = element.get(\"href\")\n",
        "\n",
        "            if URLnext is None:\n",
        "                parent_element = element.getparent()\n",
        "                # child_element = element.getchildren()  # Cueently there are no children. Therefore can't use it, otherwise will give an error.\n",
        "\n",
        "                URLnext_parent = parent_element.get(\"href\")\n",
        "                #                 URLnext_child = child_element.get(\"href\")\n",
        "\n",
        "                if URLnext_parent is not None:\n",
        "                    URLnext = URLnext_parent\n",
        "                    return URLnext\n",
        "\n",
        "                #                 elif URLnext_child is not None:\n",
        "                #                     URLnext = URLnext_child\n",
        "                #                     return URLnext\n",
        "\n",
        "                else:\n",
        "                    raise Exception(\n",
        "                        f\"No nextpage found! Check the XPath query syntex in {xpath_URLnext}.\"\n",
        "                    )\n",
        "\n",
        "            else:\n",
        "                return URLnext\n",
        "\n",
        "        else:\n",
        "            raise Exception(\n",
        "                f\"No nextpage found. Check the given XPath query syntex in {xpath_URLnext}.\"\n",
        "            )\n",
        "\n",
        "    except:\n",
        "        raise Exception(\n",
        "            f\"Desired (full/partial) class not found!! Check the XPath query syntex in {xpath_URLnext}.\"\n",
        "        )"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-07-16T15:35:50.918743Z",
          "iopub.status.busy": "2021-07-16T15:35:50.917747Z",
          "iopub.status.idle": "2021-07-16T15:35:50.939689Z",
          "shell.execute_reply": "2021-07-16T15:35:50.938689Z",
          "shell.execute_reply.started": "2021-07-16T15:35:50.918743Z"
        },
        "tags": []
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DF to dict"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "def df_to_dictDF(entity_df: \"DF\") -> dict:\n",
        "    \"\"\"\n",
        "    Break a big df into multiple small small ones. The idea is to break the df when a 'NaN' value column is present, because that's just a table heading \n",
        "    (see the original table from moneycontrol.com). Fisrt, two separate list are generated consisting all the column names and their respective location \n",
        "    (column number). These are the columns that consist only 'NaN' values. Then, a sub-df is sliced from an immediate next column where 1st all 'NaN' \n",
        "    value column is found until the 2nd 'NaN' column and so on. These 'NaN' column names become the keys for the respective sub-dfs in the returning \n",
        "    dictionary.\n",
        "\n",
        "    Parameters:\n",
        "        entity_df (DF): Input big DataFrame to be broken into small DFs.\n",
        "\n",
        "    Returns:\n",
        "        dct (dict): A dict containing DFs as values with their headers as keys.\n",
        "\n",
        "    Example:\n",
        "        new_dct = df_to_dictDF(entity_df=data)\n",
        "    \"\"\"\n",
        "\n",
        "    NaN_col_names_list = entity_df.columns[entity_df.isna().all(axis=0)].to_list()\n",
        "    NaN_col_loc_list = [\n",
        "        entity_df.columns.get_loc(c)\n",
        "        for c in entity_df.columns[entity_df.isna().all(axis=0)].to_list()\n",
        "    ]\n",
        "\n",
        "    dct = {}\n",
        "    empty_df_list = []  # If no empty DF is found then...\n",
        "\n",
        "    if len(NaN_col_loc_list) > 0:\n",
        "        for aa in range(len(NaN_col_loc_list)):\n",
        "            if aa < len(NaN_col_loc_list) - 1:\n",
        "                dct1 = {\n",
        "                    NaN_col_names_list[aa]: entity_df.iloc[\n",
        "                        :, NaN_col_loc_list[aa] + 1 : NaN_col_loc_list[aa + 1]\n",
        "                    ]\n",
        "                }\n",
        "\n",
        "                if dct1[NaN_col_names_list[aa]].empty == False:\n",
        "                    dct.update(dct1)  # update the master dict with a new dict\n",
        "                    non_empty_df_list = [*dct1]\n",
        "\n",
        "                else:\n",
        "                    empty_df_list = [*dct1]\n",
        "\n",
        "            # The last 'NaN' column does not have RHS limit, hence change in slicing condition\n",
        "            elif aa == len(NaN_col_loc_list) - 1:\n",
        "                dct2 = {\n",
        "                    NaN_col_names_list[aa]: entity_df.iloc[\n",
        "                        :, NaN_col_loc_list[aa] + 1 :\n",
        "                    ]\n",
        "                }\n",
        "\n",
        "                if dct2[NaN_col_names_list[aa]].empty == False:\n",
        "                    dct.update(dct2)  # update the master dict with the last dict\n",
        "                    non_empty_df_list.append([*dct2])\n",
        "\n",
        "                else:\n",
        "                    empty_df_list.append([*dct2])\n",
        "\n",
        "    elif len(NaN_col_loc_list) == 0:\n",
        "        dct = {entity_df.columns.name: entity_df}\n",
        "\n",
        "    else:\n",
        "        raise Exception(\"Something went wrong!!\")\n",
        "\n",
        "    keys_list = [*dct]\n",
        "\n",
        "    print(f\"sub-DF keys: {keys_list}\")\n",
        "\n",
        "    return dct"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-07-16T15:35:53.124503Z",
          "iopub.status.busy": "2021-07-16T15:35:53.124503Z",
          "iopub.status.idle": "2021-07-16T15:35:53.146443Z",
          "shell.execute_reply": "2021-07-16T15:35:53.145446Z",
          "shell.execute_reply.started": "2021-07-16T15:35:53.124503Z"
        },
        "tags": []
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import module\n",
        "\n",
        "* Investpy retrieves data from the financial products such as: stocks, funds, ETFs, indices and currency crosses, retrieved from investing.com."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "def import_hist_data(\n",
        "    product_name: str,\n",
        "    entities_dct: dict,\n",
        "    country_name: str,\n",
        "    strt_date: str,\n",
        "    end_date: str,\n",
        "    interval: str = \"daily\",\n",
        "    currency_cross: str = \"usd/inr\",\n",
        "    as_json=False,\n",
        "    order: str = \"ascending\",\n",
        "):\n",
        "    \"\"\"\n",
        "    A UDF to fetch historic data of stocks, commodities, MFs, ETFs, indices, and currencies from 13 different countries (India, US, Spain, etc.) using \n",
        "    investpy python package (which fetchs data from investing.com). The fetched data each entity is stored in a dictionary as a DF (dict value) against \n",
        "    the respective entity ticker (see investing.com for ticker symbols) as dict keys.\n",
        "\n",
        "    Parameters:\n",
        "        product_name (str): Investment options such as tocks, commodities, MFs, ETFs, indices, and currencies.\n",
        "        entities_dct (dict): Ticker symbols as keys and their respective labels (description) as values.\n",
        "        country_name (str): Currently the pyckage supports historical data for total 13 countries.\n",
        "        strt_date (str): Starting date of the fetched data.\n",
        "        end_date (str): Ending date of the fetched data.\n",
        "        interval (str): Interval between two consecutive values of the historic data. It could be seconds, minutes, hours,\n",
        "                        days (daily), weekly, monthly, quarterly, halfyearly, yearly.\n",
        "        currency_cross (str): Forex rate between two currencies when the product_name is 'currency'. E.g. \"usd/inr\".\n",
        "        as_json (bool): If True, the fetched data will be stored in as_json format.\n",
        "        order (str): The ascendig/discending order of the fetched data on date axis.\n",
        "\n",
        "    Returns:\n",
        "        hist_dct (dict): A dictionary of DFs (as dict value) against the respective entity tickers as dict keys.\n",
        "\n",
        "    Example:\n",
        "        stcks_dct = import_data(product_name=\"stocks\", entity_dict={\"HDBK\": \"HDFC Bank\"}, country_name=\"India\",\n",
        "                                strt_date=\"01/01/2019\", end_date=date.today().strftime(\"%d/%m/%Y\"), interval=\"daily\")\n",
        "    \"\"\"\n",
        "\n",
        "    product_db_stocks = [\"stock\", \"stocks\", \"equity\", \"equities\", \"share\", \"shares\"]\n",
        "\n",
        "    product_db_commodities = [\"commodity\", \"commodities\"]\n",
        "\n",
        "    product_db_MFs = [\n",
        "        \"fund\",\n",
        "        \"funds\",\n",
        "        \"mutual fund\",\n",
        "        \"mutual funds\",\n",
        "        \"mutual_fund\",\n",
        "        \"mutual_funds\",\n",
        "        \"mf\",\n",
        "        \"mfs\",\n",
        "    ]\n",
        "\n",
        "    product_db_ETFs = [\"etfs\", \"etf\"]\n",
        "\n",
        "    product_db_indices = [\"index\", \"indices\"]\n",
        "\n",
        "    product_db_currencies = [\"currencies\", \"currency\"]\n",
        "\n",
        "    # concatenate all individual lists into one.\n",
        "    product_db = (\n",
        "        product_db_stocks\n",
        "        + product_db_commodities\n",
        "        + product_db_MFs\n",
        "        + product_db_ETFs\n",
        "        + product_db_indices\n",
        "        + product_db_currencies\n",
        "    )\n",
        "\n",
        "    if product_name.lower() in product_db_commodities:\n",
        "        historical_data_dfs_list = [\n",
        "            investpy.commodities.get_commodity_historical_data(  # 'investpy.get_commodity_historical_data' also works!\n",
        "                commodity=entity,\n",
        "                country=country_name,\n",
        "                from_date=strt_date,\n",
        "                to_date=end_date,\n",
        "                interval=interval,\n",
        "                as_json=as_json,\n",
        "                order=order,\n",
        "            )\n",
        "            for entity in [*entities_dct.values()]  # list of (Commodity) DFs\n",
        "        ]\n",
        "\n",
        "    elif product_name.lower() in product_db_stocks:\n",
        "        historical_data_dfs_list = [\n",
        "            investpy.stocks.get_stock_historical_data(  # 'investpy.get_commodity_historical_data' also works!\n",
        "                stock=entity,\n",
        "                country=country_name,\n",
        "                from_date=strt_date,\n",
        "                to_date=end_date,\n",
        "                interval=interval,\n",
        "                as_json=as_json,\n",
        "                order=order,\n",
        "            ).drop(\n",
        "                columns=[\"Currency\"]\n",
        "            )\n",
        "            for entity in [*entities_dct.values()]\n",
        "        ]\n",
        "\n",
        "    elif product_name.lower() in product_db_MFs:\n",
        "        historical_data_dfs_list = [\n",
        "            investpy.funds.get_fund_historical_data(  # 'investpy.get_commodity_historical_data' also works!\n",
        "                fund=entity,\n",
        "                country=country_name,\n",
        "                from_date=strt_date,\n",
        "                to_date=end_date,\n",
        "                interval=interval,\n",
        "                as_json=as_json,\n",
        "                order=order,\n",
        "            )\n",
        "            for entity in [*entities_dct.values()]\n",
        "        ]\n",
        "\n",
        "    elif product_name.lower() in product_db_ETFs:\n",
        "        historical_data_dfs_list = [\n",
        "            investpy.etfs.get_etf_historical_data(  # 'investpy.get_commodity_historical_data' also works!\n",
        "                etf=entity,  # assign values from a dictionary to fetch data\n",
        "                country=country_name,\n",
        "                from_date=strt_date,\n",
        "                to_date=end_date,\n",
        "                interval=interval,\n",
        "                as_json=as_json,\n",
        "                order=order,\n",
        "            )\n",
        "            for entity in [*entities_dct.values()]\n",
        "        ]\n",
        "\n",
        "    elif product_name.lower() in product_db_indices:\n",
        "        historical_data_dfs_list = [\n",
        "            investpy.indices.get_index_historical_data(  # 'investpy.get_commodity_historical_data' also works!\n",
        "                index=entity,\n",
        "                country=country_name,\n",
        "                from_date=strt_date,\n",
        "                to_date=end_date,\n",
        "                interval=interval,\n",
        "                as_json=as_json,\n",
        "                order=order,\n",
        "            )\n",
        "            for entity in [*entities_dct.values()]\n",
        "        ]\n",
        "\n",
        "    elif product_name.lower() in product_db_currencies:\n",
        "        historical_data_dfs_list = [\n",
        "            investpy.currency_crosses.get_currency_cross_historical_data(  # 'investpy.get_commodity_historical_data' also works!\n",
        "                currency_cross=entity,\n",
        "                from_date=strt_date,\n",
        "                to_date=end_date,\n",
        "                interval=interval,\n",
        "                as_json=as_json,\n",
        "                order=order,\n",
        "            )\n",
        "            for entity in [*entities_dct.values()]\n",
        "        ]\n",
        "\n",
        "    else:\n",
        "        raise Exception(\n",
        "            f\"The entered product name ({product_name.lower()}) is not avalable in {product_db}\"\n",
        "        )\n",
        "\n",
        "    hist_dct = {\n",
        "        [*entities_dct.keys()][aa]: historical_data_dfs_list[aa]\n",
        "        for aa in range(len(entities_dct))\n",
        "    }\n",
        "\n",
        "    printmd(\n",
        "        f\"Historical {product_name.lower()} data for {[*entities_dct]} from {strt_date} to {end_date} is imported.\"\n",
        "    )\n",
        "    return hist_dct"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-07-16T15:35:55.679389Z",
          "iopub.status.busy": "2021-07-16T15:35:55.679389Z",
          "iopub.status.idle": "2021-07-16T15:35:55.712299Z",
          "shell.execute_reply": "2021-07-16T15:35:55.711303Z",
          "shell.execute_reply.started": "2021-07-16T15:35:55.679389Z"
        },
        "tags": []
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def stocks_financials(\n",
        "    entities_dct: dict, xpath_URLnext: str, xpath_table: str, order: str = \"ascending\"\n",
        ") -> dict:\n",
        "\n",
        "    \"\"\"\n",
        "    Scrap quarterly stock results data from the given URL (as a dict). Surprizingly, the XPath is same for \"Next\" button for any financial data on \n",
        "    moneycontrol.com! All the DFs scraped from different webpages for one stock will be merged into one with discarding repeating columns and in a further \n",
        "    usable way. Then the whole DF is segragated in smaller DFs as per their headings and are stored into a dictionary. The respective headings are then \n",
        "    keys for the respective DFs.\n",
        "\n",
        "    Parameters:\n",
        "        entities_dct (dict): Name of the entities as keys and list of their financial data URLs as values.\n",
        "        xpath_URLnext (str): XPath for pagination (\"Next\" button link)\n",
        "        xpath_table (str): XPath for table to be scraped.\n",
        "        order (str): The ascending/descending order of data on Date index in all DFs.\n",
        "\n",
        "    Returns:\n",
        "        financial_dct (dict): A dict with entity names as keys and their financial data as values. The financial data are\n",
        "                              also nested dicts having keys as \"Quarterly results, capital structure, etc...\" and respective\n",
        "                              DFs as values, and so on.\n",
        "\n",
        "    Example:\n",
        "        financial_data = stocks_financials(entities_dct=stocks_financials_dct, xpath_URLnext=xpath_financials_URLnext,                                                  xpath_table=xpath_financials_table, order=\"ascending\")\n",
        "    \"\"\"\n",
        "\n",
        "    financial_dct = {}\n",
        "\n",
        "    printmd(\"**Scraping financial data for...**\\n\", \"yellow\")\n",
        "    for entity in entities_dct.keys():\n",
        "\n",
        "        printmd(entity, \"yellow\")\n",
        "\n",
        "        entity_URLs_list = entities_dct[entity]\n",
        "\n",
        "        quarterly_results_dct = {}\n",
        "        yearly_results_dct = {}\n",
        "        balance_sheet_dct = {}\n",
        "        profit_loss_dct = {}\n",
        "        cash_flow_dct = {}\n",
        "        ratios_dct = {}\n",
        "\n",
        "        for URL in entity_URLs_list:\n",
        "            url_list = []  # List of URLs to be iterated for indefinite iteration.\n",
        "            df_list = []  # Empty list before each iteration.\n",
        "\n",
        "            if \"capital-structure\" in URL:\n",
        "\n",
        "                # Find 'tr' tag from the element-tree by given xpath.\n",
        "                # The '.' at the beginning means, that the current processing starts at the current node. The '*' selects all element nodes descending from this current node with the @id-attribute-value equal to 'mctable1'.\n",
        "                # Both child (/) and descendant-or-self (//) are axes in XPath. '/' is short for '/child::node()/'. Use '/' to select a node's immediate children. '//' is short for '/descendant-or-self::node()/'. Use '//' to select a node, its children, its grandchildren, and so on recursively.\n",
        "                mc_financial_tree = request_lxml_etree(url=URL)\n",
        "                table_element = etree_table(\n",
        "                    etree=mc_financial_tree, xpath_table=xpath_table\n",
        "                )\n",
        "\n",
        "                # Find 'tr' tag from the element-tree by given xpath.\n",
        "                trow_elements_list = table_element.findall(\".//tr\")\n",
        "\n",
        "                table = [\n",
        "                    row.xpath(\".//th//text() | .//td//text()\")\n",
        "                    for row in trow_elements_list\n",
        "                ]\n",
        "\n",
        "                # Rename the existing columns and select desired entries (rows).\n",
        "                col_names_list = [\n",
        "                    \"From\",\n",
        "                    \"Year\",\n",
        "                    \"Instrument\",\n",
        "                    \"Authorized Capital (Cr.)\",\n",
        "                    \"Issued Capital (Cr.)\",\n",
        "                    \"Shares\",\n",
        "                    \"FV\",\n",
        "                    \"Paid Capital (Cr.)\",\n",
        "                ]\n",
        "                df_raw = DF(table[2:], columns=col_names_list)\n",
        "\n",
        "                # Add the oldest year to \"Year\" column. Eg. 1994 at the bottom of DF to get rid of \"To\" and \"From\" columns in next steps.\n",
        "                df_row = pd.concat(\n",
        "                    [\n",
        "                        df_raw,\n",
        "                        DF(\n",
        "                            np.array(\n",
        "                                [\n",
        "                                    [\n",
        "                                        np.nan,\n",
        "                                        df_raw.iloc[-1, 0],\n",
        "                                        np.nan,\n",
        "                                        np.nan,\n",
        "                                        np.nan,\n",
        "                                        np.nan,\n",
        "                                        np.nan,\n",
        "                                        np.nan,\n",
        "                                    ]\n",
        "                                ]\n",
        "                            ),\n",
        "                            columns=df_raw.columns,\n",
        "                        ),\n",
        "                    ],\n",
        "                    axis=0,\n",
        "                    ignore_index=True,\n",
        "                )\n",
        "\n",
        "                # Create a new column named 'Date' with adding 'Day=1' and 'Month = March'.\n",
        "                df_row[\"Date\"] = pd.to_datetime(\n",
        "                    df_row[[\"Year\"]].assign(Day=1, Month=3), format=\"%d-%m-%Y\"\n",
        "                )\n",
        "\n",
        "                columns_drop = [\"From\", \"Year\", \"Instrument\"]\n",
        "                df = df_row.drop(columns=columns_drop).set_index(\"Date\")\n",
        "                df = df.rename_axis(entity, axis=1)\n",
        "\n",
        "                # Choose the indexing order\n",
        "                if order == \"descending\":\n",
        "                    capital_structure_df = df.sort_index(ascending=False)\n",
        "\n",
        "                else:\n",
        "                    capital_structure_df = df.sort_index(ascending=True)\n",
        "\n",
        "                printmd(f\"Capital structure data is scraped Successfully! \\n\\n\")\n",
        "\n",
        "            else:\n",
        "                # Keep fetching the data until the next page link is active.\n",
        "                while URL != \"javascript:void();\":\n",
        "                    # --------------------------- Find desired data and save it in a DF ---------------------------#\n",
        "                    mc_financial_tree = request_lxml_etree(url=URL)\n",
        "                    table_element = etree_table(\n",
        "                        etree=mc_financial_tree, xpath_table=xpath_table\n",
        "                    )\n",
        "\n",
        "                    # Find 'tr' tag from the element-tree by given xpath.\n",
        "                    trow_elements_list = table_element.findall(\".//tr\")\n",
        "\n",
        "                    # Double list comprehension to get each tr as a list of td (string)\n",
        "                    table_raw = [\n",
        "                        [td.text for td in row.getchildren()]\n",
        "                        for row in trow_elements_list\n",
        "                    ]\n",
        "\n",
        "                    # remove None table rows.\n",
        "                    table_noNone = [tdata for tdata in table_raw if tdata[0]]\n",
        "\n",
        "                    # Replace None and '\\xa0' with np.nan within TRs\n",
        "                    table = [\n",
        "                        [\n",
        "                            np.nan if any([tdata == None, tdata == \"\\xa0\"]) else tdata\n",
        "                            for tdata in trow\n",
        "                        ]\n",
        "                        for trow in table_noNone\n",
        "                    ]\n",
        "\n",
        "                    # Return DataFrame with duplicate rows removed.\n",
        "                    df_raw = DF(data=table).drop_duplicates()\n",
        "\n",
        "                    # Remove last row named as 'yrc' (date) if present.\n",
        "                    raw_drop = df_raw[df_raw[0] == \"yrc\"].index\n",
        "                    df_yrc_drop = df_raw.drop(raw_drop)\n",
        "\n",
        "                    # Replace '--' with 'missing' instead of NaN in order to separate the DF in future at NaN values columns.\n",
        "                    df_missing = df_yrc_drop.iloc[:, :].replace(\"--\", \"missing\")\n",
        "\n",
        "                    # Drop columns which have only Nan values.\n",
        "                    df_na_drop = df_missing.dropna(axis=1, how=\"all\")\n",
        "\n",
        "                    # Convert numerical strings to np.float64; non-number strings will remain as it is.\n",
        "                    df = df_na_drop.apply(\n",
        "                        pd.to_numeric, downcast=\"float\", errors=\"ignore\"\n",
        "                    )\n",
        "\n",
        "                    df_list.append(df)\n",
        "\n",
        "                    # ------------------------ Get subsequent URLs for the same data until the end ------------------------ #\n",
        "                    url_list.append(URL)  # List of subsequent URL.\n",
        "                    urlnext = etree_URLnext(\n",
        "                        etree=mc_financial_tree, xpath_URLnext=xpath_URLnext\n",
        "                    )\n",
        "                    URL = urlnext\n",
        "\n",
        "                # Concat individual df from all pages into one DF\n",
        "                df_concated = pd.concat(df_list, axis=1, ignore_index=True)\n",
        "\n",
        "                # Remove '(a) , (b) , (c) , i) , ii) , - ', etc. elements from each data (string).\n",
        "                string_to_replace = \"\\(a\\) |\\(b\\) |\\(c\\) |\\(d\\) |a\\) |b\\) |c\\) |d\\) |i\\) |ii\\) |'|- | :|\\.$\"\n",
        "                string_replaced_with = \"\"\n",
        "                df_str_fmtd = df_concated.replace(\n",
        "                    string_to_replace, string_replaced_with, regex=True\n",
        "                )\n",
        "\n",
        "                # Make 1st row as column header and drop the 1st raw.\n",
        "                df_str_fmtd.columns = df_str_fmtd.iloc[0]\n",
        "                df_fmtd_hdr = df_str_fmtd.drop(df_str_fmtd.index[0])\n",
        "\n",
        "                # Remove duplicate columns that is nedded once but is present on every page (e.g. 'Quarterly Results of HDFC Bank (in Rs. Cr.)').\n",
        "                data_raw = df_fmtd_hdr.loc[\n",
        "                    :, ~df_fmtd_hdr.columns.duplicated()\n",
        "                ]  # It checks the column header!\n",
        "                data_raw.set_index(\n",
        "                    data_raw.columns[0], inplace=True\n",
        "                )  # set 1st column as index\n",
        "                data_trsps = data_raw.transpose()  # Transpose the df\n",
        "\n",
        "                # Format string date to datetime object.\n",
        "                data_trsps.index = pd.to_datetime(\n",
        "                    data_trsps.index, format=\"%b %y\", dayfirst=True\n",
        "                )\n",
        "                data_trsps.index.name = \"Date\"  # Assign name to the index column.\n",
        "\n",
        "                # Rename the index title with (a shorter) equity name.\n",
        "                data_trsps_index = data_trsps.rename_axis(entity, axis=1)\n",
        "\n",
        "                # Choose the indexing order\n",
        "                if order == \"descending\":\n",
        "                    data = data_trsps_index.sort_index(ascending=False)\n",
        "\n",
        "                else:\n",
        "                    data = data_trsps_index.sort_index(ascending=True)\n",
        "\n",
        "                if \"quarterly-results\" in url_list[0]:\n",
        "                    if \"consolidated\" not in url_list[0]:\n",
        "                        printmd(\n",
        "                            f\"Standalone Quarterly Result data from {len(url_list)} urls is scraped Successfully!\"\n",
        "                        )\n",
        "\n",
        "                        # Here \"update\" is needed because 'standalone' and 'consolidated' are two values in the same dict.\n",
        "                        # Break big DF into samll ones and save them as a dict.\n",
        "                        quarterly_results_dct.update(\n",
        "                            {\"Standalone\": df_to_dictDF(entity_df=data)}\n",
        "                        )\n",
        "\n",
        "                    elif \"consolidated\" in url_list[0]:\n",
        "                        printmd(\n",
        "                            f\"Consolidated Quarterly Result data from {len(url_list)} urls is scraped Successfully!\"\n",
        "                        )\n",
        "                        quarterly_results_dct.update(\n",
        "                            {\"Consolidated\": df_to_dictDF(entity_df=data)}\n",
        "                        )\n",
        "\n",
        "                elif \"yearly\" in url_list[0]:\n",
        "                    if \"consolidated\" not in url_list[0]:\n",
        "                        printmd(\n",
        "                            f\"Standalone Yearly Result data from {len(url_list)} urls is scraped Successfully!\"\n",
        "                        )\n",
        "                        yearly_results_dct.update(\n",
        "                            {\"Standalone\": df_to_dictDF(entity_df=data)}\n",
        "                        )\n",
        "\n",
        "                    elif \"consolidated\" in url_list[0]:\n",
        "                        printmd(\n",
        "                            f\"Consolidated Yearly Result data from {len(url_list)} urls is scraped Successfully!\"\n",
        "                        )\n",
        "                        yearly_results_dct.update(\n",
        "                            {\"Consolidated\": df_to_dictDF(entity_df=data)}\n",
        "                        )\n",
        "\n",
        "                elif \"balance-sheet\" in url_list[0]:\n",
        "                    if \"consolidated\" not in url_list[0]:\n",
        "                        printmd(\n",
        "                            f\"Standalone Balance-Sheet data from {len(url_list)} urls is scraped Successfully!\"\n",
        "                        )\n",
        "                        balance_sheet_dct.update(\n",
        "                            {\"Standalone\": df_to_dictDF(entity_df=data)}\n",
        "                        )\n",
        "\n",
        "                    elif \"consolidated\" in url_list[0]:\n",
        "                        printmd(\n",
        "                            f\"Consolidated Balance-Sheet data from {len(url_list)} urls is scraped Successfully!\"\n",
        "                        )\n",
        "                        balance_sheet_dct.update(\n",
        "                            {\"Consolidated\": df_to_dictDF(entity_df=data)}\n",
        "                        )\n",
        "\n",
        "                elif \"profit-loss\" in url_list[0]:\n",
        "                    if \"consolidated\" not in url_list[0]:\n",
        "                        printmd(\n",
        "                            f\"Standalone Profit/Loss data from {len(url_list)} urls is scraped Successfully!\"\n",
        "                        )\n",
        "                        profit_loss_dct.update(\n",
        "                            {\"Standalone\": df_to_dictDF(entity_df=data)}\n",
        "                        )\n",
        "\n",
        "                    elif \"consolidated\" in url_list[0]:\n",
        "                        printmd(\n",
        "                            f\"Consolidated Profit/Loss data from {len(url_list)} urls is scraped Successfully!\"\n",
        "                        )\n",
        "                        profit_loss_dct.update(\n",
        "                            {\"Consolidated\": df_to_dictDF(entity_df=data)}\n",
        "                        )\n",
        "\n",
        "                elif \"cash-flow\" in url_list[0]:\n",
        "                    if \"consolidated\" not in url_list[0]:\n",
        "                        printmd(\n",
        "                            f\"Standalone Cash-flow data from {len(url_list)} urls is scraped Successfully!\"\n",
        "                        )\n",
        "\n",
        "                        cash_flow_dct.update(\n",
        "                            {\"Standalone\": df_to_dictDF(entity_df=data)}\n",
        "                        )\n",
        "\n",
        "                    elif \"consolidated\" in url_list[0]:\n",
        "                        printmd(\n",
        "                            f\"Consolidated Cash-flow data from {len(url_list)} urls is scraped Successfully!\"\n",
        "                        )\n",
        "                        cash_flow_dct.update(\n",
        "                            {\"Consolidated\": df_to_dictDF(entity_df=data)}\n",
        "                        )\n",
        "\n",
        "                elif \"ratios\" in url_list[-2]:\n",
        "                    if \"consolidated\" not in url_list[0]:\n",
        "                        printmd(\n",
        "                            f\"Standalone Ratio data from {len(url_list)} urls is scraped Successfully!\"\n",
        "                        )\n",
        "                        ratios_dct.update({\"Standalone\": df_to_dictDF(entity_df=data)})\n",
        "\n",
        "                    elif \"consolidated\" in url_list[0]:\n",
        "                        printmd(\n",
        "                            f\"Consolidated Ratio data from {len(url_list)} urls is scraped Successfully!\"\n",
        "                        )\n",
        "                        ratios_dct.update(\n",
        "                            {\"Consolidated\": df_to_dictDF(entity_df=data)}\n",
        "                        )\n",
        "\n",
        "                else:\n",
        "                    raise Exception(\"Extend the code!!!!!\")\n",
        "\n",
        "        # Update a dict with the product name as a key and a dict of corresponding sub-DFs corresponding to their respective heading.\n",
        "        financial_dct.update(\n",
        "            {\n",
        "                entity: {\n",
        "                    \"Balance Sheet\": balance_sheet_dct,\n",
        "                    \"Profit/Loss\": profit_loss_dct,\n",
        "                    \"Quarterly Results\": quarterly_results_dct,\n",
        "                    \"Yearly Results\": yearly_results_dct,\n",
        "                    \"Cash Flows\": cash_flow_dct,\n",
        "                    \"Ratios\": ratios_dct,\n",
        "                    \"Capital Structure\": capital_structure_df,\n",
        "                }\n",
        "            }\n",
        "        )\n",
        "\n",
        "    printmd(\n",
        "        \"------------------------------ Scraping Successfull! ------------------------------\"\n",
        "    )\n",
        "    return financial_dct"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-07-16T15:35:57.358138Z",
          "iopub.status.busy": "2021-07-16T15:35:57.358138Z",
          "iopub.status.idle": "2021-07-16T15:35:57.408004Z",
          "shell.execute_reply": "2021-07-16T15:35:57.407006Z",
          "shell.execute_reply.started": "2021-07-16T15:35:57.358138Z"
        },
        "tags": []
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def stocks_company_facts(\n",
        "    entities_dct: dict, xpath_table: str, order: str = \"ascending\"\n",
        ") -> dict:\n",
        "    \"\"\"\n",
        "    Scrap company facts from the given URL (as a dict). Surprizingly the XPath is same for \"Next\" button for any stock! Thereafter, the all DFs scraped \n",
        "    from different webpages for one stock will be merged into one with discarding repeating columns and in a further usable way.\n",
        "\n",
        "    Parameters:\n",
        "        entities_dct (dict): Name of the entities as keys and list of their facts data URLs as values.\n",
        "        xpath_table (str): XPath for table to be scraped.\n",
        "        order (str): The ascending/descending order of data on Date index in all DFs.\n",
        "\n",
        "    Returns:\n",
        "        company_facts_dct (dict): A dict with entity names as keys and their facts as values. The financial data are also\n",
        "                                  nested dicts having keys as \"bonus, right, etc...\" and respective DFs as values, and so on.\n",
        "\n",
        "    Example:\n",
        "        stocks_company_facts_data = stocks_company_facts(entities_dct=stocks_company_facts_dct,\n",
        "        xpath_table=companyfacts_xpath_table, order=\"ascending\")\n",
        "    \"\"\"\n",
        "\n",
        "    company_facts_dct = {}\n",
        "\n",
        "    printmd(\"**Scraping Company facts data for...**\\n\\n\", \"yellow\")\n",
        "\n",
        "    for entity in entities_dct.keys():\n",
        "        printmd(entity, \"yellow\")\n",
        "\n",
        "        URLs = entities_dct[entity]\n",
        "\n",
        "        for URL in URLs:\n",
        "            # --------------------------- Find interested data and save it in a DF ---------------------------#\n",
        "            mc_companyfacts_tree = request_lxml_etree(url=URL)  # mc: moneycontrol\n",
        "            table_element = etree_table(\n",
        "                etree=mc_companyfacts_tree, xpath_table=xpath_table\n",
        "            )\n",
        "\n",
        "            # Find 'tr' tag from the element-tree by given xpath.\n",
        "            trow_elements_list = table_element.findall(\".//tr\")\n",
        "\n",
        "            # Double list comprehension to get each tr as a list of td (string)\n",
        "            table_raw = [\n",
        "                [td.text for td in row.getchildren()] for row in trow_elements_list\n",
        "            ]\n",
        "\n",
        "            traw_trs_noNone = [\n",
        "                table_row for table_row in table_raw if len(table_row) != 0\n",
        "            ]  # remove None trs.\n",
        "\n",
        "            # Replace None and '\\xa0' with np.nan within TRs\n",
        "            table = [\n",
        "                [\n",
        "                    np.nan if any([tdata == None, tdata == \"\\xa0\"]) else tdata\n",
        "                    for tdata in trow\n",
        "                ]\n",
        "                for trow in traw_trs_noNone\n",
        "            ]\n",
        "\n",
        "            # Return DataFrame with duplicate rows removed.\n",
        "            df_raw = DF(data=table)  # .drop_duplicates()\n",
        "\n",
        "            # Convert numerical strings to np.float64; non-number strings will remain as it is.\n",
        "            df = df_raw.apply(pd.to_numeric, downcast=\"float\", errors=\"ignore\")\n",
        "\n",
        "            # Remove '(a) , (b) , (c) , i) , ii) , - ', etc. elements from each data (string).\n",
        "            string_to_replace = \"\\\\t|\\(a\\) |\\(b\\) |\\(c\\) |\\(d\\) |a\\) |b\\) |c\\) |d\\) |i\\) |ii\\) |'|- | :|\\.$\"\n",
        "            string_replaced_with = \"\"\n",
        "            df_str_fmtd = df.replace(\n",
        "                string_to_replace, string_replaced_with, regex=True\n",
        "            )\n",
        "\n",
        "            # Make 1st row as column header and drop the 1st raw.\n",
        "            df_str_fmtd.columns = df_str_fmtd.iloc[0]\n",
        "            df_fmtd_hdr = df_str_fmtd.drop(df_str_fmtd.index[0])\n",
        "\n",
        "            index_name = df_fmtd_hdr.columns[df_fmtd_hdr.columns.str.match(\"^E\")][0]\n",
        "            df_fmtd_hdr.set_index(index_name, inplace=True)  # set 1st column as index\n",
        "\n",
        "            # Format string date to datetime object.\n",
        "            df_fmtd_hdr.index = pd.to_datetime(df_fmtd_hdr.index, dayfirst=True)\n",
        "            df_fmtd_hdr.index.name = index_name  # Assign name to the index column.\n",
        "\n",
        "            # Rename the index title with (a shorter) equity name.\n",
        "            df = df_fmtd_hdr.rename_axis(entity, axis=1)\n",
        "\n",
        "            # Choose the index order\n",
        "            if order == \"descending\":\n",
        "                data = df.sort_index(ascending=False)\n",
        "\n",
        "            else:\n",
        "                data = df.sort_index(ascending=True)\n",
        "\n",
        "            if \"bonus\" in URL:\n",
        "                printmd(\"Bonus data is scraped Successfully!\")\n",
        "                bonus_df = data\n",
        "\n",
        "            elif \"rights\" in URL:\n",
        "                printmd(\"Rights data is scraped Successfully!\")\n",
        "                rights_df = data\n",
        "\n",
        "            elif \"splits\" in URL:\n",
        "                printmd(\"Splits data is scraped Successfully!\")\n",
        "                splits_df = data\n",
        "\n",
        "            elif \"dividends\" in URL:\n",
        "                printmd(\"Dividends data is scraped Successfully! \\n\\n\")\n",
        "                dividends_df = data\n",
        "\n",
        "            else:\n",
        "                raise Exception(\"Extend the code!!!!!\")\n",
        "\n",
        "        company_facts_dct.update(\n",
        "            {\n",
        "                entity: {\n",
        "                    \"Bonus\": bonus_df,\n",
        "                    \"Rights\": rights_df,\n",
        "                    \"Splits\": splits_df,\n",
        "                    \"Dividends\": dividends_df,\n",
        "                }\n",
        "            }\n",
        "        )\n",
        "\n",
        "    print(\n",
        "        \"---------------------------------- Scraping Successfull! ----------------------------------\"\n",
        "    )\n",
        "\n",
        "    return company_facts_dct"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-07-16T15:35:59.701358Z",
          "iopub.status.busy": "2021-07-16T15:35:59.700392Z",
          "iopub.status.idle": "2021-07-16T15:35:59.729285Z",
          "shell.execute_reply": "2021-07-16T15:35:59.728319Z",
          "shell.execute_reply.started": "2021-07-16T15:35:59.701358Z"
        },
        "tags": []
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Operations on one or among two/three columns"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "def operation_col_concat(\n",
        "    entities_dct: dict,\n",
        "    oprtn_name: str,\n",
        "    resltn_col_name: str,\n",
        "    oprnd_col1: str,\n",
        "    oprnd_col2: str = None,\n",
        "    oprnd_col3: str = None,\n",
        "    constant: float = None,\n",
        "    forecast_time: str = None,\n",
        "    window_size: int = None,\n",
        "    window_type: str = None,  # extend or rolling\n",
        ") -> dict:\n",
        "    \"\"\"\n",
        "    Perform a \"single\" operation at a time on one or among two/three columns of all DFs in the input dict. For percentage, the difference between the 1st \n",
        "    and 2nd column is divided by the 3rd column. The result of the operation will be stored in a new DF with a column name (resltn_col_name) and is \n",
        "    concatenated to the original DF.\n",
        "\n",
        "    Parameters:\n",
        "        entities_dct (dict): A dictionary of imported historical data.\n",
        "        oprtn_name (str): Name of the opertation that is going to be performed on operand column(s).\n",
        "        resltn_col_name (str): Title of the resultant column.\n",
        "        oprn_col(1,2,3) (str): Name of the column on which the operation is going to be performed.\n",
        "        constant (numeral): A constant value.\n",
        "        forecast_time (numeral): No. of (hours, days, months) to be forecasted.\n",
        "\n",
        "    Returns:\n",
        "        new_dict (dict): A new dictionary consisting the original input dictinary concated with the resultant column.\n",
        "\n",
        "    Example:\n",
        "        stk_frcst_dct = operation_col_concat(entity_dict = stk_dct, oprtn_name = \"predict\", resltn_col_name =\n",
        "                        \"Close_predict\", oprnd_col1 = \"Close\", oprnd_col2 = None, oprnd_col3 = None, constant = None,\n",
        "                        forecast_time = 30)\n",
        "    \"\"\"\n",
        "    oprtn_mean = [\"average\", \"avg\", \"mean\"]\n",
        "    oprtn_VWAP = [\"vwap\", \"volume weighted average price\"]\n",
        "    oprtn_add = [\"addition\", \"add\", \"sum\"]\n",
        "    oprtn_sub = [\"subtraction\", \"sub\", \"minus\", \"difference\"]\n",
        "    oprtn_div = [\"division\", \"divide\", \"div\"]\n",
        "    oprtn_mul = [\"multiplication\", \"mul\", \"product\"]\n",
        "    oprtn_prdct = [\"predict\", \"prediction\", \"forecast\", \"future\"]\n",
        "    oprtn_pct = [\"percentage\", \"percent\"]\n",
        "    oprtn_rtrn = [\n",
        "        \"percentage change\",\n",
        "        \"percentage_change\",\n",
        "        \"percent change\",\n",
        "        \"percent_change\",\n",
        "        \"pct change\",\n",
        "        \"pct_change\",\n",
        "        \"%\",\n",
        "        \"% change\",\n",
        "        \"%_change\",\n",
        "        \"return\",\n",
        "    ]\n",
        "    oprtn_sdv = [\n",
        "        \"standard deviation\",\n",
        "        \"standard_deviation\",\n",
        "        \"sigma\",\n",
        "        \"std\",\n",
        "        \"sdv\",\n",
        "        \"std deviation\",\n",
        "        \"std_deviation\",\n",
        "        \"std dvtn\",\n",
        "        \"std_dvtn\",\n",
        "    ]\n",
        "    oprtn_var = [\"variance\"]\n",
        "\n",
        "    oprtn_names = (\n",
        "        oprtn_add\n",
        "        + oprtn_sub\n",
        "        + oprtn_div\n",
        "        + oprtn_mul\n",
        "        + oprtn_prdct\n",
        "        + oprtn_pct\n",
        "        + oprtn_rtrn\n",
        "        + oprtn_var\n",
        "        + oprtn_sdv\n",
        "        + oprtn_VWAP\n",
        "        + oprtn_mean\n",
        "    )\n",
        "\n",
        "    col_names = [*entities_dct.values()][0].columns\n",
        "\n",
        "    if isinstance(entities_dct, dict):\n",
        "        # Operation name and the operand column1 should always be defines.\n",
        "        if (oprtn_name.lower() in oprtn_names) and (oprnd_col1 in col_names):\n",
        "\n",
        "            if oprtn_name.lower() in oprtn_VWAP:\n",
        "                if (constant, forecast_time is None) and (oprnd_col2 in col_names):\n",
        "                    new_dict = {\n",
        "                        [*entities_dct.keys()][aa]: pd.concat(\n",
        "                            [\n",
        "                                [*entities_dct.values()][aa],\n",
        "                                DF(\n",
        "                                    (\n",
        "                                        (\n",
        "                                            (\n",
        "                                                [*entities_dct.values()][aa][oprnd_col1]\n",
        "                                                * [*entities_dct.values()][aa][\n",
        "                                                    oprnd_col2\n",
        "                                                ]\n",
        "                                            )\n",
        "                                            .expanding(min_periods=2)\n",
        "                                            .sum(axis=1)\n",
        "                                            / [*entities_dct.values()][aa][oprnd_col2]\n",
        "                                            .expanding(min_periods=2)\n",
        "                                            .sum(axis=1)\n",
        "                                        )\n",
        "                                        * 100\n",
        "                                    ),\n",
        "                                    columns=[resltn_col_name],\n",
        "                                ),\n",
        "                            ],\n",
        "                            axis=1,\n",
        "                        )\n",
        "                        for aa in range(len([*entities_dct]))\n",
        "                    }\n",
        "\n",
        "                else:\n",
        "                    raise Exception(\n",
        "                        f\"Three opearand columns are nedded to carry out {oprtn_name} operation.\"\n",
        "                    )\n",
        "\n",
        "            elif oprtn_name.lower() in oprtn_sdv:\n",
        "                if (oprnd_col2, oprnd_col3, constant, forecast_time is None):\n",
        "\n",
        "                    # In rolling function the window size remain constant whereas in the expanding the window gets bigger by the min_period. Format: (N1) = Standard deviation; (N1, N2) = standard deviation; (N1, N2, N3) = standard deviation.\n",
        "                    #  Delta Degrees of Freedom (ddof). The divisor used in calculations is N-ddof, where N represents the number of elements.\n",
        "                    new_dict = {\n",
        "                        [*entities_dct.keys()][aa]: pd.concat(\n",
        "                            [\n",
        "                                [*entities_dct.values()][aa],\n",
        "                                DF(\n",
        "                                    (\n",
        "                                        [*entities_dct.values()][aa][oprnd_col1]\n",
        "                                        .expanding(min_periods=2)\n",
        "                                        .std(axis=1, numeric_only=true, ddof=1)\n",
        "                                    ),\n",
        "                                    columns=[\n",
        "                                        resltn_col_name\n",
        "                                    ],  # ddof = 1 means N-1 elements used to divide.\n",
        "                                ),\n",
        "                            ],\n",
        "                            axis=1,\n",
        "                        )\n",
        "                        for aa in range(len([*entity_dict]))\n",
        "                    }\n",
        "\n",
        "                else:\n",
        "                    raise Exception(\n",
        "                        f\"Something went wrong! The {oprtn_name} could not be performed on {oprnd_col1}.\"\n",
        "                    )\n",
        "\n",
        "            elif oprtn_name.lower() in oprtn_var:\n",
        "                if (oprnd_col2, oprnd_col3, constant, forecast_time is None):\n",
        "                    new_dict = {\n",
        "                        [*entities_dct.keys()][aa]: pd.concat(\n",
        "                            [\n",
        "                                [*entities_dct.values()][aa],\n",
        "                                DF(\n",
        "                                    (\n",
        "                                        [*entities_dct.values()][aa][oprnd_col1]\n",
        "                                        .expanding(min_periods=2)\n",
        "                                        .var(axis=1, ddof=1)\n",
        "                                    ),\n",
        "                                    columns=[resltn_col_name],\n",
        "                                ),\n",
        "                            ],\n",
        "                            axis=1,\n",
        "                        )\n",
        "                        for aa in range(len([*entities_dct]))\n",
        "                    }\n",
        "\n",
        "                else:\n",
        "                    raise Exception(\n",
        "                        f\"Something went wrong! The {oprtn_name} could not be performed on {oprnd_col1}.\"\n",
        "                    )\n",
        "\n",
        "            elif oprtn_name.lower() in oprtn_rtrn:\n",
        "                if (oprnd_col2, oprnd_col3, constant, forecast_time is None):\n",
        "                    new_dict = {\n",
        "                        [*entities_dct.keys()][aa]: pd.concat(\n",
        "                            [\n",
        "                                [*entities_dct.values()][aa],\n",
        "                                DF(\n",
        "                                    (\n",
        "                                        [*entities_dct.values()][aa][\n",
        "                                            oprnd_col1\n",
        "                                        ].pct_change()\n",
        "                                        * 100\n",
        "                                    ),\n",
        "                                    columns=[resltn_col_name],\n",
        "                                ),\n",
        "                            ],\n",
        "                            axis=1,\n",
        "                        )\n",
        "                        for aa in range(len([*entities_dct]))\n",
        "                    }\n",
        "\n",
        "                else:\n",
        "                    raise Exception(\n",
        "                        f\"Something went wrong! The {oprtn_name} could not be performed on {oprnd_col1}.\"\n",
        "                    )\n",
        "\n",
        "            elif oprtn_name.lower() in oprtn_pct:\n",
        "                if (constant, forecast_time is None) and (\n",
        "                    oprnd_col2,\n",
        "                    oprnd_col3 in col_names,\n",
        "                ):\n",
        "                    new_dict = {\n",
        "                        [*entities_dct.keys()][aa]: pd.concat(\n",
        "                            [\n",
        "                                [*entities_dct.values()][aa],\n",
        "                                DF(\n",
        "                                    (\n",
        "                                        (\n",
        "                                            (\n",
        "                                                [*entities_dct.values()][aa][oprnd_col1]\n",
        "                                                - [*entities_dct.values()][aa][\n",
        "                                                    oprnd_col2\n",
        "                                                ]\n",
        "                                            )\n",
        "                                            / [*entities_dct.values()][aa][oprnd_col3]\n",
        "                                        )\n",
        "                                        * 100\n",
        "                                    ),\n",
        "                                    columns=[resltn_col_name],\n",
        "                                ),\n",
        "                            ],\n",
        "                            axis=1,\n",
        "                        )\n",
        "                        for aa in range(len([*entities_dct]))\n",
        "                    }\n",
        "\n",
        "                else:\n",
        "                    raise Exception(\n",
        "                        f\"Three opearand columns are nedded to carry out {oprtn_name} operation.\"\n",
        "                    )\n",
        "\n",
        "            elif oprtn_name.lower() in oprtn_add:\n",
        "                if (constant, oprnd_col3, forecast_time is None) and (\n",
        "                    oprnd_col2 in col_names\n",
        "                ):\n",
        "                    new_dict = {\n",
        "                        [*entities_dct.keys()][aa]: pd.concat(\n",
        "                            [\n",
        "                                [*entities_dct.values()][aa],\n",
        "                                DF(\n",
        "                                    (\n",
        "                                        [*entities_dct.values()][aa][oprnd_col1]\n",
        "                                        + [*entities_dct.values()][aa][oprnd_col2]\n",
        "                                    ),\n",
        "                                    columns=[resltn_col_name],\n",
        "                                ),\n",
        "                            ],\n",
        "                            axis=1,\n",
        "                        )\n",
        "                        for aa in range(len([*entities_dct]))\n",
        "                    }\n",
        "\n",
        "                elif (constant is not None) and (\n",
        "                    oprnd_col2,\n",
        "                    oprnd_col3,\n",
        "                    forecast_time is None,\n",
        "                ):\n",
        "                    new_dict = {\n",
        "                        [*entities_dct.keys()][aa]: pd.concat(\n",
        "                            [\n",
        "                                [*entities_dct.values()][aa],\n",
        "                                [*entities_dct.values()][aa][oprnd_col1]\n",
        "                                .add(constant)\n",
        "                                .to_frame(name=resltn_col_name),\n",
        "                            ],\n",
        "                            axis=1,\n",
        "                        )\n",
        "                        for aa in range(len([*entities_dct]))\n",
        "                    }\n",
        "\n",
        "                else:\n",
        "                    raise Exception(\n",
        "                        f\"Something went wrong! The {oprtn_name} operation could not be performed!\"\n",
        "                    )\n",
        "\n",
        "            elif oprtn_name.lower() in oprtn_sub:\n",
        "                if (oprnd_col3, constant, forecast_time is None) and (\n",
        "                    oprnd_col2 in col_names\n",
        "                ):\n",
        "                    new_dict = {\n",
        "                        [*entities_dct.keys()][aa]: pd.concat(\n",
        "                            [\n",
        "                                [*entities_dct.values()][aa],\n",
        "                                DF(\n",
        "                                    (\n",
        "                                        [*entities_dct.values()][aa][oprnd_col1]\n",
        "                                        - [*entities_dct.values()][aa][oprnd_col2]\n",
        "                                    ),\n",
        "                                    columns=[resltn_col_name],\n",
        "                                ),\n",
        "                            ],\n",
        "                            axis=1,\n",
        "                        )\n",
        "                        for aa in range(len([*entities_dct]))\n",
        "                    }\n",
        "\n",
        "                elif (constant is not None) and (\n",
        "                    oprnd_col2,\n",
        "                    oprnd_col3,\n",
        "                    forecast_time is None,\n",
        "                ):\n",
        "                    new_dict = {\n",
        "                        [*entities_dct.keys()][aa]: pd.concat(\n",
        "                            [\n",
        "                                [*entities_dct.values()][aa],\n",
        "                                [*entities_dct.values()][aa][oprnd_col1]\n",
        "                                .add(constant)\n",
        "                                .to_frame(name=resltn_col_name),\n",
        "                            ],\n",
        "                            axis=1,\n",
        "                        )\n",
        "                        for aa in range(len([*entities_dct]))\n",
        "                    }\n",
        "\n",
        "                else:\n",
        "                    raise Exception(\n",
        "                        f\"Something went wrong! The {oprtn_name} operation could not be performed!\"\n",
        "                    )\n",
        "\n",
        "            elif oprtn_name.lower() in oprtn_mul:\n",
        "                if (constant, oprnd_col3, forecast_time is None) and (\n",
        "                    oprnd_col2 in col_names\n",
        "                ):\n",
        "                    new_dict = {\n",
        "                        [*entities_dct.keys()][aa]: pd.concat(\n",
        "                            [\n",
        "                                [*entities_dct.values()][aa],\n",
        "                                DF(\n",
        "                                    (\n",
        "                                        [*entities_dct.values()][aa][oprnd_col1]\n",
        "                                        * [*entities_dct.values()][aa][oprnd_col2]\n",
        "                                    ),\n",
        "                                    columns=[resltn_col_name],\n",
        "                                ),\n",
        "                            ],\n",
        "                            axis=1,\n",
        "                        )\n",
        "                        for aa in range(len([*entities_dct]))\n",
        "                    }\n",
        "\n",
        "                elif (constant is not None) and (\n",
        "                    oprnd_col2,\n",
        "                    oprnd_col3,\n",
        "                    forecast_time is None,\n",
        "                ):\n",
        "                    new_dict = {\n",
        "                        [*entities_dct.keys()][aa]: pd.concat(\n",
        "                            [\n",
        "                                [*entities_dct.values()][aa],\n",
        "                                [*entities_dct.values()][aa][oprnd_col1]\n",
        "                                .mul(constant)\n",
        "                                .to_frame(name=resltn_col_name),\n",
        "                            ],\n",
        "                            axis=1,\n",
        "                        )\n",
        "                        for aa in range(len([*entity_dict]))\n",
        "                    }\n",
        "\n",
        "                else:\n",
        "                    raise Exception(\n",
        "                        f\"Something went wrong! The {oprtn_name} operation could not be performed!\"\n",
        "                    )\n",
        "\n",
        "            elif oprtn_name.lower() in oprtn_div:\n",
        "                if (constant, forecast_time, oprnd_col3 is None) and (\n",
        "                    oprnd_col2 in col_names\n",
        "                ):\n",
        "                    new_dict = {\n",
        "                        [*entities_dct.keys()][aa]: pd.concat(\n",
        "                            [\n",
        "                                [*entities_dct.values()][aa],\n",
        "                                DF(\n",
        "                                    (\n",
        "                                        [*entities_dct.values()][aa][oprnd_col1]\n",
        "                                        / [*entities_dct.values()][aa][oprnd_col2]\n",
        "                                    ),\n",
        "                                    columns=[resltn_col_name],\n",
        "                                ),\n",
        "                            ],\n",
        "                            axis=1,\n",
        "                        )\n",
        "                        for aa in range(len([*entities_dct]))\n",
        "                    }\n",
        "\n",
        "                elif ((constant is not None) and (constant != 0)) and (\n",
        "                    oprnd_col2,\n",
        "                    oprnd_col3,\n",
        "                    forecast_time is None,\n",
        "                ):\n",
        "                    new_dict = {\n",
        "                        [*entities_dct.keys()][aa]: pd.concat(\n",
        "                            [\n",
        "                                [*entities_dct.values()][aa],\n",
        "                                [*entities_dct.values()][aa][oprnd_col1]\n",
        "                                .div(constant)\n",
        "                                .to_frame(name=resltn_col_name),\n",
        "                            ],\n",
        "                            axis=1,\n",
        "                        )\n",
        "                        for aa in range(len([*entities_dct]))\n",
        "                    }\n",
        "\n",
        "                else:\n",
        "                    raise Exception(\"Cannot divide by 0.\")\n",
        "\n",
        "            elif oprtn_name.lower() in oprtn_prdct:\n",
        "                if (constant, oprnd_col2, oprnd_col3 is None) and (\n",
        "                    (forecast_time != 0) or (forecast_time is not None)\n",
        "                ):\n",
        "                    new_dict = {\n",
        "                        [*entities_dct.keys()][aa]: pd.concat(\n",
        "                            [\n",
        "                                [*entities_dct.values()][aa],\n",
        "                                [*entities_dct.values()][aa][oprnd_col1]\n",
        "                                .shift(-forecast_time)  # forecast days from today.\n",
        "                                .to_frame(name=resltn_col_name),\n",
        "                            ],\n",
        "                            axis=1,\n",
        "                        )\n",
        "                        for aa in range(len(entities_dct))\n",
        "                    }\n",
        "\n",
        "                else:\n",
        "                    pass\n",
        "\n",
        "            else:\n",
        "                raise Exception(\n",
        "                    f\"Something went wrong! The {oprtn_name} operation could not be performed!\"\n",
        "                )\n",
        "\n",
        "            printmd(\n",
        "                f\"The '{oprtn_name}' operation on all the DFs of the input dict has been preformed and the resultant column ({resltn_col_name}) is concated to all the DFs respectively.\"\n",
        "            )\n",
        "\n",
        "            return new_dict\n",
        "\n",
        "        else:\n",
        "            raise Exception(\n",
        "                f\"Either the entered operation name {oprtn_name} does not match with the availebale operations {oprtn_names} or the entered operand column names {oprnd_col1, oprnd_col2, oprnd_col3} do not match with the entity's column names {col_names}.\"\n",
        "            )\n",
        "\n",
        "    else:\n",
        "        raise Exception(\n",
        "            f\"The input entity must be of a dictionary type not {type(entities_dct)}.\"\n",
        "        )"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-07-16T15:36:01.519560Z",
          "iopub.status.busy": "2021-07-16T15:36:01.518562Z",
          "iopub.status.idle": "2021-07-16T15:36:01.586377Z",
          "shell.execute_reply": "2021-07-16T15:36:01.585380Z",
          "shell.execute_reply.started": "2021-07-16T15:36:01.519560Z"
        },
        "tags": []
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Transpose imported data"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "def col_entity_dict(entities_dct: dict) -> dict:\n",
        "    \"\"\"\n",
        "    Returns a dictionary with all the columns as keys and the data with respect to that particular column from all the entities collected in a DF. For \n",
        "    e.g. the imported data is a dictionary and has a form of {entity_name: value_df([col1(Open)]) [col2 (High)] [col3 (Low)] [col4 (Close)], [col5 \n",
        "    (Volume)], [col6 (Currency)])}. This dictionary is converted in a new (can I  say transposed!) dictionary of the form, {col1('Open'): \n",
        "    value_df([entity_name1 (col1)] [entity_name2 (col1)][entity_name3 (col1)]...... [entity_nameN (col1)])}.\n",
        "\n",
        "    Parameters:\n",
        "        entities_dct (dict): Input dictionary.\n",
        "\n",
        "    Returns:\n",
        "        entity_cols_dct (dict): A transposed dictionary with rearranfed data where the columns of DFs (values in input dict)\n",
        "        becomes the keys and the ticker symbols (keys of input dict) becomes the column names of a DF corresponding to the new\n",
        "        keys. E.g. entity_dct = {\"SBI\": DF(\"Close\", \"Open\"), \"HDBK\": DF(\"Close\", \"Open\")} becomes... entity_cols_dct =\n",
        "                                {\"Close\": DF(\"SBI\", \"HDBK\"), \"Open\": DF(\"SBI\", \"HDBK\")}.\n",
        "\n",
        "    Example:\n",
        "        stk_col_dct = col_entity_dict(entity_dict = stk_dct)\n",
        "    \"\"\"\n",
        "\n",
        "    if isinstance(entities_dct, dict):\n",
        "        entity_cols_dct = {}\n",
        "        for column in [*entity_dct.values()][\n",
        "            0\n",
        "        ].columns:  # list column names of any DF (here first) to make them keys\n",
        "            entity_col_df = pd.concat(\n",
        "                [entity[column] for entity in entities_dct.values()], axis=1\n",
        "            )\n",
        "            entity_col_df.columns = [entity_name for entity_name in [*entities_dct]]\n",
        "            entity_cols_dct[column] = entity_column_df\n",
        "\n",
        "        printmd(\n",
        "            f\"A dictionary of historical data from {entity_col_df.index[0].date()} to {entity_col_df.index[-1].date()} is sorted by column names ({[*[*entities_dct.values()][0].columns]}) and these new columns of the DFs are renamed as 'entity_name (col)' ({[*entity_col_df.columns]}).\"\n",
        "        )\n",
        "\n",
        "        return entity_cols_dict\n",
        "\n",
        "    else:\n",
        "        raise Exception(\n",
        "            f\"The {entities_dct} must be of dict type where it has {type(entities_dct)}.\"\n",
        "        )"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-07-16T15:36:03.332173Z",
          "iopub.status.busy": "2021-07-16T15:36:03.332173Z",
          "iopub.status.idle": "2021-07-16T15:36:03.348092Z",
          "shell.execute_reply": "2021-07-16T15:36:03.347094Z",
          "shell.execute_reply.started": "2021-07-16T15:36:03.332173Z"
        },
        "tags": []
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Select column(s) from dict of DFs"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "def select_cols(entities_dct: dict, col_names: list) -> dict:\n",
        "    \"\"\"\n",
        "    A UDF to select particular column(s) from the DFs in a dictionary returning in a new dict with the same format.\n",
        "\n",
        "    Parameters:\n",
        "        entities_dct (dict): Input dictionary.\n",
        "        col_names (str): List of column names to be extracted.\n",
        "\n",
        "    Returns:\n",
        "        selected_cols_dict (dict): A dictionray with only selected column(s) in DFs.\n",
        "\n",
        "    Example:\n",
        "        stk_Cls_Opn_dct = select_cols(entity_dict= stk_dct, col_names = [\"Close\", \"Open\"])\n",
        "    \"\"\"\n",
        "    if isinstance(entities_dct, dict):\n",
        "        selected_cols_dct = {\n",
        "            [*entities_dct][aa]: [*entities_dct.values()][aa][col_names]\n",
        "            for aa in range(len(entities_dct))\n",
        "        }\n",
        "        printmd(\n",
        "            f\"The {col_names} column(s) are extracted from the DFs of input dict. The extracted columns are returned as DFs in a new dict. with the same keys.\"\n",
        "        )\n",
        "        return selected_cols_dct\n",
        "\n",
        "    else:\n",
        "        raise Exception(\n",
        "            f\"The {entities_dct} must be of dict type where it has {type(entities_dct)}.\"\n",
        "        )"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-07-16T15:36:05.532332Z",
          "iopub.status.busy": "2021-07-16T15:36:05.531337Z",
          "iopub.status.idle": "2021-07-16T15:36:05.545299Z",
          "shell.execute_reply": "2021-07-16T15:36:05.544332Z",
          "shell.execute_reply.started": "2021-07-16T15:36:05.532332Z"
        },
        "tags": []
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Window filters to DF column(s) in a transpose dict"
      ],
      "metadata": {
        "heading_collapsed": "true",
        "tags": []
      }
    },
    {
      "cell_type": "raw",
      "source": [
        "def window_filters_cols(entity_dict, key_name, win_weights, win_size, interval=\"days\", center=False, axis=0):\n",
        "    \"\"\"\n",
        "    \n",
        "    \"\"\"\n",
        "    if isinstance(entity_dict, dict):\n",
        "\n",
        "        # A copy is needed otherwise the same input dict is modified. It creates unwanted results when the same modified dict is undergone for filtering again and again.\n",
        "        entity_dict_copy = {}\n",
        "        entity_dict_copy1 = {}\n",
        "        new_dict = {}\n",
        "\n",
        "        key_names = [*entity_dict, \"all\"]  # Key list of entity plus the 'all'.\n",
        "\n",
        "        if win_weights is None:\n",
        "            win_name = \"mavg\"\n",
        "\n",
        "        else:\n",
        "            win_name = win_wieghts\n",
        "\n",
        "        if key_name == \"all\":\n",
        "            keys = [*entity_dict]\n",
        "            entity_name = [*entity_dict.values()][0].columns\n",
        "\n",
        "            for key in keys:\n",
        "                entity_dict_copy[key] = entity_dict[key].select_dtypes(\n",
        "                    exclude=\"object\"\n",
        "                )  # Discard DFs of string dtypes in a new dict\n",
        "                filtered_df = (\n",
        "                    entity_dict_copy[key]\n",
        "                    .rolling(window=win_size, center=center, win_type=win_weights, axis=axis)\n",
        "                    .mean()\n",
        "                )\n",
        "                filtered_df.columns = [\n",
        "                    column + f\" ({win_name} = {win_size} {interval})\" for column in filtered_df.columns\n",
        "                ]\n",
        "\n",
        "                entity_dict_copy[key] = filtered_df\n",
        "                entity_dict_copy1[key] = entity_dict[key].select_dtypes(\n",
        "                    include=\"object\"\n",
        "                )  # Add DFs of string dtypes in a new dict\n",
        "\n",
        "                new_dict[key] = pd.concat([entity_dict_copy[key], entity_dict_copy1[key]], axis=1)\n",
        "\n",
        "        elif key_name in [*entity_dict]:\n",
        "            filtered_df = (\n",
        "                entity_dict[key_name].rolling(window=win_size, center=center, win_type=win_weights, axis=axis).mean()\n",
        "            )\n",
        "            filtered_df.columns = [column + f\" ({win_name} = {win_size} {interval})\" for column in filtered_df.columns]\n",
        "            new_dict_ = entity_dict.copy()\n",
        "            new_dict[key_name] = filtered_df\n",
        "\n",
        "        else:\n",
        "            raise Exception(f\"The entered column name {key_name} not found. It must be from {key_names}.\")\n",
        "\n",
        "        printmd(\n",
        "            f\"The input data has been filtered using a rolling window of '{win_name}' with window size of {win_size} and returned as a dict with updated column name(s) as col ({win_name} = {win_size} {interval}).\"\n",
        "        )\n",
        "\n",
        "        return new_dict\n",
        "\n",
        "    else:\n",
        "        raise Exception(f\"The input data must be of dict type where it has {type(entity_dict)}.\")"
      ],
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-02-01T11:47:14.233508Z",
          "iopub.status.busy": "2021-02-01T11:47:14.233508Z",
          "iopub.status.idle": "2021-02-01T11:47:14.244501Z",
          "shell.execute_reply": "2021-02-01T11:47:14.243502Z",
          "shell.execute_reply.started": "2021-02-01T11:47:14.233508Z"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Date difference"
      ],
      "metadata": {
        "tags": []
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def days_bw(forecast_date: str, strt_datetime: \"date\" = date.today()) -> int:\n",
        "    \"\"\"\n",
        "    A UDF to find between two dates The start date will be today by default but can be changed.\n",
        "\n",
        "    Parameters:\n",
        "        forecast_date (str): The future date untill which the forecast will take place. E.g. \"20/12/2020\"\n",
        "        strt_datetime (date): Start date time.\n",
        "\n",
        "    Returns:\n",
        "        days (int): An integer defference between the forecast date and the start date.\n",
        "\n",
        "    Example:\n",
        "        forecast_days = days_bw(forecast_date=\"01/01/2019\", strt_datetime = date.today())\n",
        "    \"\"\"\n",
        "    days = (forecast_date - strt_datetime).days\n",
        "\n",
        "    if days > 0:\n",
        "        printmd(f\"The input dates will forecst for next {days} days\", \"green\")\n",
        "\n",
        "    elif days == 0:\n",
        "        raise Exception(\"The difference between dates is 0 days, which makes no sense!\")\n",
        "\n",
        "    else:\n",
        "        warn(f\"WARNING: The input dates will 'retrodict' for last {abs(days)} days.\")\n",
        "\n",
        "    return days"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-07-16T15:36:11.395893Z",
          "iopub.status.busy": "2021-07-16T15:36:11.394897Z",
          "iopub.status.idle": "2021-07-16T15:36:11.404869Z",
          "shell.execute_reply": "2021-07-16T15:36:11.403874Z",
          "shell.execute_reply.started": "2021-07-16T15:36:11.395893Z"
        },
        "tags": []
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def moneycontrol_urls(entities_dct: dict, product: str, categories_list: list) -> dict:\n",
        "    \"\"\"\n",
        "    A UDF returning a dict of entity names as keys and the list of respective product URLs as values.\n",
        "\n",
        "    Parameters:\n",
        "        entities_dct (dict): Input dictionary.\n",
        "        product (str): 'financials', 'company-facts'.\n",
        "\n",
        "    Returns:\n",
        "        url_dct (dict): A dictionray with only selected column(s) in DFs.\n",
        "\n",
        "    Example:\n",
        "        moneycontrol_stk_url_dct = moneycontrol_urls(entities_dct=stocks_url_dct, product=product, categories_list=categories)\n",
        "    \"\"\"\n",
        "\n",
        "    base_url = \"https://www.moneycontrol.com/\"\n",
        "    url_dct = {}\n",
        "    for entity_key, entity_value in entities_dct.items():\n",
        "        url_list = []\n",
        "        for category in categories_list:\n",
        "            url = base_url + product + \"/\" + entity_value + \"/\" + category + \"/\"\n",
        "            url_list.append(url)\n",
        "\n",
        "        url_dct.update({entity_key: url_list})\n",
        "\n",
        "    return url_dct"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-07-16T15:36:13.369987Z",
          "iopub.status.busy": "2021-07-16T15:36:13.368990Z",
          "iopub.status.idle": "2021-07-16T15:36:13.388936Z",
          "shell.execute_reply": "2021-07-16T15:36:13.387939Z",
          "shell.execute_reply.started": "2021-07-16T15:36:13.369987Z"
        },
        "tags": []
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Stock Analysis\n",
        "### Import and modify data\n",
        "#### Input parameters"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Historical Price Data"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "start_date = \"01/01/2019\"\n",
        "end_date = date.today().strftime(\"%d/%m/%Y\")\n",
        "interval = \"Daily\"\n",
        "\n",
        "stocks_dct = {\"HDFC Bank\": \"HDBK\", \"SBI Bank\": \"SBI\"}  # product_sector_objectType"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-07-16T15:36:18.354017Z",
          "iopub.status.busy": "2021-07-16T15:36:18.354017Z",
          "iopub.status.idle": "2021-07-16T15:36:18.360996Z",
          "shell.execute_reply": "2021-07-16T15:36:18.359999Z",
          "shell.execute_reply.started": "2021-07-16T15:36:18.354017Z"
        },
        "tags": []
      }
    },
    {
      "cell_type": "code",
      "source": [
        "s_bk_dt = import_hist_data(\n",
        "    product_name=\"stocks\",\n",
        "    entities_dct=stocks_dct,\n",
        "    country_name=\"India\",\n",
        "    strt_date=start_date,\n",
        "    end_date=end_date,\n",
        "    interval=interval,\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-07-16T15:36:21.025407Z",
          "iopub.status.busy": "2021-07-16T15:36:21.024410Z",
          "iopub.status.idle": "2021-07-16T15:36:22.729845Z",
          "shell.execute_reply": "2021-07-16T15:36:22.728847Z",
          "shell.execute_reply.started": "2021-07-16T15:36:21.025407Z"
        },
        "tags": []
      }
    },
    {
      "cell_type": "code",
      "source": [
        "display(s_bk_dt[\"HDFC Bank\"])"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-07-16T15:36:31.258128Z",
          "iopub.status.busy": "2021-07-16T15:36:31.258128Z",
          "iopub.status.idle": "2021-07-16T15:36:31.399687Z",
          "shell.execute_reply": "2021-07-16T15:36:31.398687Z",
          "shell.execute_reply.started": "2021-07-16T15:36:31.258128Z"
        },
        "tags": []
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Historical Financial Data"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "xpath_financials_URLnext = './/span[contains(@class, \"nextpag\")]'\n",
        "xpath_financials_table = './/table[contains(@class, \"mctable\")]'  # \"/html/body/section/div[2]/div/div[2]/div[2]/div/div[2]/div/div[1]/table\"\n",
        "\n",
        "companyfacts_xpath_table = './/table[contains(@class, \"mctable\")]'\n",
        "\n",
        "stocks_url_dct = {\"SBI Bank\": \"statebankindia\", \"HDFC Bank\": \"hdfcbank\"}\n",
        "\n",
        "finance_prdct = \"financials\"\n",
        "companyfacts_prdct = \"company-facts\"\n",
        "\n",
        "finance_ctgrs_list = [\n",
        "    \"balance-sheetVI\",\n",
        "    \"consolidated-balance-sheetVI\",\n",
        "    \"profit-lossVI\",\n",
        "    \"consolidated-profit-lossVI\",\n",
        "    \"results/quarterly-results\",\n",
        "    \"results/consolidated-quarterly-results\",\n",
        "    \"results/yearly\",\n",
        "    \"results/consolidated-yearly\",\n",
        "    \"cash-flowVI\",\n",
        "    \"consolidated-cash-flowVI\",\n",
        "    \"ratiosVI\",\n",
        "    \"consolidated-ratiosVI\",\n",
        "    \"capital-structure/SBI\",\n",
        "]\n",
        "\n",
        "companyfacts_ctgrs_list = [\"bonus\", \"rights\", \"splits\", \"dividends\"]"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-07-16T15:36:41.791610Z",
          "iopub.status.busy": "2021-07-16T15:36:41.790604Z",
          "iopub.status.idle": "2021-07-16T15:36:41.808553Z",
          "shell.execute_reply": "2021-07-16T15:36:41.806584Z",
          "shell.execute_reply.started": "2021-07-16T15:36:41.791610Z"
        },
        "tags": []
      }
    },
    {
      "cell_type": "code",
      "source": [
        "url_finance_dct = moneycontrol_urls(\n",
        "    entities_dct=stocks_url_dct,\n",
        "    product=finance_prdct,\n",
        "    categories_list=finance_ctgrs_list,\n",
        ")\n",
        "url_companyfacts_dct = moneycontrol_urls(\n",
        "    entities_dct=stocks_url_dct,\n",
        "    product=companyfacts_prdct,\n",
        "    categories_list=companyfacts_ctgrs_list,\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-07-16T15:36:43.557088Z",
          "iopub.status.busy": "2021-07-16T15:36:43.557088Z",
          "iopub.status.idle": "2021-07-16T15:36:43.575003Z",
          "shell.execute_reply": "2021-07-16T15:36:43.574005Z",
          "shell.execute_reply.started": "2021-07-16T15:36:43.557088Z"
        },
        "tags": []
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stocks_financials_data = stocks_financials(\n",
        "    entities_dct=url_finance_dct,\n",
        "    xpath_URLnext=xpath_financials_URLnext,\n",
        "    xpath_table=xpath_financials_table,\n",
        "    order=\"ascending\",\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-07-16T15:36:45.242694Z",
          "iopub.status.busy": "2021-07-16T15:36:45.242694Z",
          "iopub.status.idle": "2021-07-16T15:38:39.018899Z",
          "shell.execute_reply": "2021-07-16T15:38:39.017901Z",
          "shell.execute_reply.started": "2021-07-16T15:36:45.242694Z"
        },
        "tags": []
      }
    },
    {
      "cell_type": "code",
      "source": [
        "display(stocks_financials_data.keys())\n",
        "display(stocks_financials_data[\"HDFC Bank\"].keys())\n",
        "display(stocks_financials_data[\"HDFC Bank\"][\"Balance Sheet\"].keys())\n",
        "display(stocks_financials_data[\"HDFC Bank\"][\"Balance Sheet\"][\"Standalone\"].keys())\n",
        "display(\n",
        "    stocks_financials_data[\"HDFC Bank\"][\"Balance Sheet\"][\"Standalone\"][\n",
        "        \"SHAREHOLDERS FUNDS\"\n",
        "    ].keys()\n",
        ")  # no more inner dicts\n",
        "display(\n",
        "    stocks_financials_data[\"HDFC Bank\"][\"Capital Structure\"].keys()\n",
        ")  # no more inner dicts"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-07-16T15:38:53.179679Z",
          "iopub.status.busy": "2021-07-16T15:38:53.179679Z",
          "iopub.status.idle": "2021-07-16T15:38:53.208600Z",
          "shell.execute_reply": "2021-07-16T15:38:53.207605Z",
          "shell.execute_reply.started": "2021-07-16T15:38:53.179679Z"
        },
        "tags": []
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Historical stock details"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "stocks_company_facts_data = stocks_company_facts(\n",
        "    entities_dct=url_companyfacts_dct,\n",
        "    xpath_table=companyfacts_xpath_table,\n",
        "    order=\"ascending\",\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-07-16T15:38:56.979221Z",
          "iopub.status.busy": "2021-07-16T15:38:56.978223Z",
          "iopub.status.idle": "2021-07-16T15:39:07.183067Z",
          "shell.execute_reply": "2021-07-16T15:39:07.182069Z",
          "shell.execute_reply.started": "2021-07-16T15:38:56.979221Z"
        },
        "tags": []
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------------------ Forecast time ------------------------------ #\n",
        "forecast_days = 30  # days\n",
        "\n",
        "# ------------------------------ Prediciton date index from today------------------------------ #\n",
        "predict_start = datetime.today()\n",
        "date_frcstd_index = pd.date_range(\n",
        "    start=predict_start,\n",
        "    end=None,\n",
        "    periods=forecast_days + 1,\n",
        "    freq=\"B\",\n",
        "    tz=None,\n",
        "    normalize=True,\n",
        "    name=date,\n",
        "    closed=\"right\",\n",
        ")  # name = DatetimeIndex; closed = None means extreme points are included; Business (B) days frequency; lower value is exluded hence forecast_days days will yield forecast_days-1 entries, therefore forecast_days+1 days.\n",
        "\n",
        "\n",
        "# ------------------------------ List of Columns in DF ------------------------------ #\n",
        "cls_col = \"Close\"\n",
        "hi_col = \"High\"\n",
        "lw_col = \"Low\"\n",
        "opn_col = \"Open\"\n",
        "COO_col = \"(C-O)/O\"\n",
        "HLC_col = \"(H-L)/C\"\n",
        "cls_rtrn = \"ΔC/C\"  # (N_2 - N_1)/N_2\n",
        "cls_predict_col = (\n",
        "    f\"{cls_col} (Φ: {forecast_days} d)\"  # Φ denotes futre or prediction (forecast time)\n",
        ")\n",
        "\n",
        "\n",
        "# ------------------------------ List of Operations ------------------------------ #\n",
        "oprtn_predict = \"predict\"  # Will just up-shift the data (entries) by forecast time and puts Nan in the blank entries.\n",
        "oprtn_pct = \"percentage\"  # For (High-Low)/Close and (Close - Open)/Open percentages.\n",
        "oprtn_std = \"standard deviation\"  # or \"std\", \"sigma\"\n",
        "oprtn_VWAP = \"volume weighted average price\"  # or \"VWAP\"\n",
        "oprtn_return = \"return\"  # or \"percentage change\""
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-07-16T15:39:10.719531Z",
          "iopub.status.busy": "2021-07-16T15:39:10.719531Z",
          "iopub.status.idle": "2021-07-16T15:39:10.817271Z",
          "shell.execute_reply": "2021-07-16T15:39:10.816273Z",
          "shell.execute_reply.started": "2021-07-16T15:39:10.719531Z"
        },
        "tags": []
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "__Investigate the model accuracy by varying the independent variables.__"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "s_bk_HLC_dt = operation_col_concat(\n",
        "    entities_dct=s_bk_dt,\n",
        "    oprtn_name=oprtn_pct,\n",
        "    resltn_col_name=HLC_col,\n",
        "    oprnd_col1=hi_col,\n",
        "    oprnd_col2=lw_col,\n",
        "    oprnd_col3=cls_col,\n",
        "    constant=None,\n",
        "    forecast_time=None,\n",
        ")\n",
        "\n",
        "s_bk_HLC_COO_dt = operation_col_concat(\n",
        "    entities_dct=s_bk_HLC_dt,\n",
        "    oprtn_name=oprtn_pct,\n",
        "    resltn_col_name=COO_col,\n",
        "    oprnd_col1=cls_col,\n",
        "    oprnd_col2=opn_col,\n",
        "    oprnd_col3=opn_col,\n",
        "    constant=None,\n",
        ")\n",
        "\n",
        "s_bk_HLC_COO_frct_dt = operation_col_concat(\n",
        "    entities_dct=s_bk_HLC_COO_dt,\n",
        "    oprtn_name=oprtn_predict,\n",
        "    resltn_col_name=cls_predict_col,\n",
        "    oprnd_col1=cls_col,\n",
        "    oprnd_col2=None,\n",
        "    oprnd_col3=None,\n",
        "    constant=None,\n",
        "    forecast_time=forecast_days,\n",
        ")\n",
        "\n",
        "display(s_bk_HLC_COO_frct_dt[\"HDFC Bank\"])"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-07-16T15:39:15.384666Z",
          "iopub.status.busy": "2021-07-16T15:39:15.384666Z",
          "iopub.status.idle": "2021-07-16T15:39:15.437523Z",
          "shell.execute_reply": "2021-07-16T15:39:15.436527Z",
          "shell.execute_reply.started": "2021-07-16T15:39:15.384666Z"
        },
        "tags": []
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import metrics, preprocessing\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.linear_model import (\n",
        "    ElasticNet,\n",
        "    ElasticNetCV,\n",
        "    Lars,\n",
        "    LarsCV,\n",
        "    Lasso,\n",
        "    LassoCV,\n",
        "    LassoLars,\n",
        "    LassoLarsCV,\n",
        "    LinearRegression,\n",
        "    MultiTaskElasticNet,\n",
        "    MultiTaskElasticNetCV,\n",
        "    MultiTaskLasso,\n",
        "    Ridge,\n",
        "    RidgeCV,\n",
        "    SGDRegressor,\n",
        ")\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score,\n",
        "    classification_report,\n",
        "    confusion_matrix,\n",
        "    r2_score,\n",
        ")\n",
        "from sklearn.model_selection import cross_validate, train_test_split\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "\n",
        "# from sklearn import cross_validation\n",
        "\n",
        "# from sklearn.preprocessing import MinMaxScaler\n",
        "# scaler = MinMaxScaler(feature_range=(0, 1))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-07-16T15:39:19.596370Z",
          "iopub.status.busy": "2021-07-16T15:39:19.595375Z"
        },
        "tags": []
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------------------ Classical linear regressors Models ------------------------------ #\n",
        "\n",
        "model_LR = LinearRegression(fit_intercept=True, normalize=False, copy_X=True, n_jobs=-1)  \n",
        "\n",
        "display(MD(\"$$LR:\\ \\hat{y}(w, x) = ω_0 + ω_1 x_1 + ω_2 x_2 + ..... + ω_n x_n$$\"))\n",
        "\n",
        "\n",
        "# Bias/variance tradeoff: the larger the ridge alpha parameter-> the higher the bias and the lower the variance (the amount of shrinkage and thus the coefficients become more robust to collinearity).\n",
        "display(MD(\"$$Ridge = \\min_{ω} || Xω - y||_2^2 + 𝛼 ||ω||_2^2$$\"))\n",
        "model_Ridge = Ridge(\n",
        "    alpha=5, fit_intercept=True, normalize=False, copy_X=True, solver=\"auto\", random_state=41,\n",
        ")  # Ridge LR\n",
        "\n",
        "model_RidgeCV = RidgeCV(alphas=(0.01, 1.0, 10.0), fit_intercept=True, normalize=False)  # RidgeCV LR\n",
        "\n",
        "\n",
        "# Not understood completely!\n",
        "model_SGDR = SGDRegressor(\n",
        "    loss=\"squared_loss\",\n",
        "    penalty=\"elasticnet\",\n",
        "    alpha=0.0001,\n",
        "    l1_ratio=0.15,\n",
        "    fit_intercept=True,\n",
        "    max_iter=1000,\n",
        "    shuffle=True,\n",
        "    random_state=41,\n",
        "    learning_rate=\"adaptive\",\n",
        ")\n",
        "\n",
        "\n",
        "# ---------------------------- Regressors with variable selection ----------------------- #\n",
        "model_ElaNet = ElasticNet(\n",
        "    alpha=1.0,\n",
        "    l1_ratio=0.5,\n",
        "    fit_intercept=True,\n",
        "    normalize=False,\n",
        "    precompute=False,\n",
        "    max_iter=1000,\n",
        "    copy_X=True,\n",
        "    tol=0.0001,\n",
        "    warm_start=False,\n",
        "    positive=False,\n",
        "    random_state=None,\n",
        "    selection=\"cyclic\",\n",
        ")\n",
        "\n",
        "model_ElaNetCV = ElasticNetCV(\n",
        "    l1_ratio=0.5,\n",
        "    eps=0.001,\n",
        "    n_alphas=100,\n",
        "    alphas=None,\n",
        "    fit_intercept=True,\n",
        "    normalize=False,\n",
        "    precompute=\"auto\",\n",
        "    max_iter=1000,\n",
        "    tol=0.0001,\n",
        "    cv=None,\n",
        "    copy_X=True,\n",
        "    verbose=0,\n",
        "    n_jobs=None,\n",
        "    positive=False,\n",
        "    random_state=None,\n",
        "    selection=\"cyclic\",\n",
        ")\n",
        "\n",
        "model_MultiTaskElasticNet = MultiTaskElasticNet(\n",
        "    alpha=1.0,\n",
        "    l1_ratio=0.5,\n",
        "    fit_intercept=True,\n",
        "    normalize=False,\n",
        "    copy_X=True,\n",
        "    max_iter=1000,\n",
        "    tol=0.0001,\n",
        "    warm_start=False,\n",
        "    random_state=None,\n",
        "    selection=\"cyclic\",\n",
        ")\n",
        "\n",
        "model_MultiTaskElasticNetCV = MultiTaskElasticNetCV(\n",
        "    l1_ratio=0.5,\n",
        "    eps=0.001,\n",
        "    n_alphas=100,\n",
        "    alphas=None,\n",
        "    fit_intercept=True,\n",
        "    normalize=False,\n",
        "    max_iter=1000,\n",
        "    tol=0.0001,\n",
        "    cv=None,\n",
        "    copy_X=True,\n",
        "    verbose=0,\n",
        "    n_jobs=None,\n",
        "    random_state=None,\n",
        "    selection=\"cyclic\",\n",
        ")\n",
        "\n",
        "model_Lasso = Lasso(alpha=5, fit_intercept=True, normalize=False, copy_X=True, random_state=41)\n",
        "\n",
        "model_LassoCV = LassoCV(\n",
        "    eps=0.001,\n",
        "    n_alphas=100,\n",
        "    alphas=None,\n",
        "    fit_intercept=True,\n",
        "    normalize=False,\n",
        "    precompute=\"auto\",\n",
        "    max_iter=1000,\n",
        "    tol=0.1,\n",
        "    copy_X=True,\n",
        "    cv=None,\n",
        "    verbose=False,\n",
        "    n_jobs=None,\n",
        "    positive=False,\n",
        "    random_state=None,\n",
        "    selection=\"cyclic\",\n",
        ")\n",
        "\n",
        "model_MultiTaskLasso = MultiTaskLasso(\n",
        "    alpha=1.0,\n",
        "    fit_intercept=True,\n",
        "    normalize=False,\n",
        "    copy_X=True,\n",
        "    max_iter=1000,\n",
        "    tol=0.0001,\n",
        "    warm_start=False,\n",
        "    random_state=None,\n",
        "    selection=\"cyclic\",\n",
        ")\n",
        "\n",
        "model_Lars = Lars(\n",
        "    fit_intercept=True,\n",
        "    verbose=False,\n",
        "    normalize=True,\n",
        "    precompute=\"auto\",\n",
        "    n_nonzero_coefs=500,\n",
        "    eps=2.220446049250313e-16,\n",
        "    copy_X=True,\n",
        "    fit_path=True,\n",
        "    jitter=None,\n",
        "    random_state=None,\n",
        ")\n",
        "\n",
        "model_LarsCV = LarsCV(\n",
        "    fit_intercept=True,\n",
        "    verbose=False,\n",
        "    max_iter=500,\n",
        "    normalize=True,\n",
        "    precompute=\"auto\",\n",
        "    cv=None,\n",
        "    max_n_alphas=1000,\n",
        "    n_jobs=None,\n",
        "    eps=2.220446049250313e-16,\n",
        "    copy_X=True,\n",
        ")\n",
        "\n",
        "# Lasso model fit with Least Angle Regression a.k.a. Lars. It is a Linear Model trained with an L1 prior as regularizer.\n",
        "model_LassoLARS = LassoLars(\n",
        "    alpha=1.0,\n",
        "    fit_intercept=True,\n",
        "    verbose=False,\n",
        "    normalize=True,\n",
        "    precompute=\"auto\",\n",
        "    max_iter=500,\n",
        "    eps=2.220446049250313e-16,\n",
        "    copy_X=True,\n",
        "    fit_path=True,\n",
        "    positive=False,\n",
        "    jitter=None,\n",
        "    random_state=None,\n",
        ")\n",
        "\n",
        "model_LassoLarsCV = LassoLarsCV(\n",
        "    fit_intercept=True,\n",
        "    verbose=False,\n",
        "    max_iter=500,\n",
        "    normalize=True,\n",
        "    precompute=\"auto\",\n",
        "    cv=None,\n",
        "    max_n_alphas=1000,\n",
        "    n_jobs=None,\n",
        "    eps=2.220446049250313e-16,\n",
        "    copy_X=True,\n",
        "    positive=False,\n",
        ")\n",
        "\n",
        "\n",
        "# ------------------------------ Nearest Neighbors Models ------------------------------ #\n",
        "model_KNN = KNeighborsRegressor(\n",
        "    n_neighbors=43,\n",
        "    weights=\"uniform\",\n",
        "    algorithm=\"auto\",\n",
        "    leaf_size=100,\n",
        "    p=2,\n",
        "    metric=\"minkowski\",\n",
        "    metric_params=None,\n",
        "    n_jobs=-1,\n",
        ")  # leaf_size?\n",
        "\n",
        "model_dist_KNN = KNeighborsRegressor(\n",
        "    n_neighbors=43,\n",
        "    weights=\"distance\",\n",
        "    algorithm=\"auto\",\n",
        "    leaf_size=5,\n",
        "    p=2,\n",
        "    metric=\"minkowski\",\n",
        "    metric_params=None,\n",
        "    n_jobs=-1,\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-03-25T09:42:28.364020Z",
          "iopub.status.busy": "2021-03-25T09:42:28.363022Z",
          "iopub.status.idle": "2021-03-25T09:42:28.418873Z",
          "shell.execute_reply": "2021-03-25T09:42:28.417876Z",
          "shell.execute_reply.started": "2021-03-25T09:42:28.364020Z"
        },
        "tags": []
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------- Seaparete the independent and dependent variables ---------------------------- #\n",
        "# Independent variables to predict CP (Just independent variables like OP, HP, LP, and volume)\n",
        "X = s_bk_HLC_COO_frct_dt[\"HDFC Bank\"].drop(columns=[cls_predict_col], axis=1)[\n",
        "    :-forecast_days\n",
        "]  # Only select data\n",
        "# display(X)\n",
        "\n",
        "# Dependent variable (CP) prediction from the independent variables\n",
        "Y = s_bk_HLC_COO_frct_dt[\"HDFC Bank\"][[cls_predict_col]][:-forecast_days]\n",
        "\n",
        "X_future = s_bk_HLC_COO_frct_dt[\"HDFC Bank\"].drop(columns=[cls_predict_col], axis=1)[\n",
        "    -forecast_days:\n",
        "]  # Time Series (TS) sequence\n",
        "\n",
        "test_size = [\n",
        "    0.5,\n",
        "    0.6,\n",
        "    0.67,\n",
        "    0.7,\n",
        "    0.75,\n",
        "]  # [0.1, 0.2, 0.25, 0.3, 0.33, 0.4, 0.5, 0.6, 0.67, 0.7, 0.75, 0.8, 0.9]"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-03-25T09:43:34.737103Z",
          "iopub.status.busy": "2021-03-25T09:43:34.736104Z",
          "iopub.status.idle": "2021-03-25T09:43:34.755052Z",
          "shell.execute_reply": "2021-03-25T09:43:34.754054Z",
          "shell.execute_reply.started": "2021-03-25T09:43:34.737103Z"
        },
        "tags": []
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_list = []\n",
        "X_test_list = []\n",
        "Y_train_list = []\n",
        "Y_test_list = []\n",
        "\n",
        "conf_LR_list = []\n",
        "conf_Ridge_list = []\n",
        "conf_RidgeCV_list = []\n",
        "conf_Lasso_list = []\n",
        "conf_SGDR_list = []\n",
        "conf_KNN_list = []\n",
        "conf_dist_KNN_list = []\n",
        "\n",
        "for aa in range(len(test_size)):\n",
        "    X_train, X_test, Y_train, Y_test = train_test_split(\n",
        "        X, Y, test_size=test_size[aa], shuffle=True, random_state=41\n",
        "    )\n",
        "\n",
        "    X_train_list.append(X_train)\n",
        "    X_test_list.append(X_test)\n",
        "    Y_train_list.append(Y_train)\n",
        "    Y_test_list.append(Y_test)\n",
        "\n",
        "    model_LR.fit(X_train_list[aa], Y_train_list[aa])  # training the algorithm\n",
        "    conf_LR = model_LR.score(X_test_list[aa], Y_test_list[aa]) * 100\n",
        "    conf_LR_list.append(conf_LR)\n",
        "    print(f\"Test size: {test_size[aa]} \\t\\t Confidence_LR: {conf_LR_list[aa]}\")\n",
        "\n",
        "    model_Ridge.fit(X_train_list[aa], Y_train_list[aa])\n",
        "    conf_Ridge = model_Ridge.score(X_test_list[aa], Y_test_list[aa]) * 100\n",
        "    conf_Ridge_list.append(conf_Ridge)\n",
        "    print(f\"Test size: {test_size[aa]} \\t\\t Confidence_Ridge: {conf_Ridge_list[aa]}\")\n",
        "\n",
        "    # For some cases the accuracy to predict data (Y_test) goes below zero.\n",
        "    #     model_RidgeCV.fit(X_train_list[aa], Y_train_list[aa])\n",
        "    #     conf_RidgeCV = model_RidgeCV.score(X_test_list[aa], Y_test_list[aa])\n",
        "    #     conf_RidgeCV_list.append(conf_RidgeCV)\n",
        "    #     print(\n",
        "    #         f\"Test size: {test_size[aa]} \\t\\t Confidence_RidgeCV: {conf_RidgeCV_list[aa]}\"\n",
        "    #     )\n",
        "\n",
        "    model_Lasso.fit(X_train_list[aa], Y_train_list[aa])\n",
        "    conf_Lasso = model_Lasso.score(X_test_list[aa], Y_test_list[aa]) * 100\n",
        "    conf_Lasso_list.append(conf_Lasso)\n",
        "    print(f\"Test size: {test_size[aa]} \\t\\t Confidence_Lasso: {conf_Lasso_list[aa]}\")\n",
        "\n",
        "    #     model_SGDR.fit(X_train_list[aa], Y_train_list[aa])\n",
        "    #     conf_SGDR = model_SGDR.score(X_test_list[aa], Y_test_list[aa])\n",
        "    #     conf_SGDR_list.append(conf_SGDR)\n",
        "    #     print(f\"Test size: {test_size[aa]} \\t\\t Confidence_SGDR: {conf_SGDR_list[aa]}\")\n",
        "\n",
        "    model_KNN.fit(X_train_list[aa], Y_train_list[aa])\n",
        "    conf_KNN = model_KNN.score(X_test_list[aa], Y_test_list[aa]) * 100\n",
        "    conf_KNN_list.append(conf_KNN)\n",
        "    print(f\"Test size: {test_size[aa]} \\t\\t Confidence_KNN: {conf_KNN_list[aa]}\")\n",
        "\n",
        "    model_dist_KNN.fit(X_train_list[aa], Y_train_list[aa])\n",
        "    conf_dist_KNN = model_dist_KNN.score(X_test_list[aa], Y_test_list[aa]) * 100\n",
        "    conf_dist_KNN_list.append(conf_dist_KNN)\n",
        "    print(\n",
        "        f\"Test size: {test_size[aa]} \\t\\t Confidence_dist_KNN: {conf_dist_KNN_list[aa]}\"\n",
        "    )\n",
        "\n",
        "    print(\"\\n\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-03-25T09:46:58.857706Z",
          "iopub.status.busy": "2021-03-25T09:46:58.857706Z",
          "iopub.status.idle": "2021-03-25T09:46:59.568805Z",
          "shell.execute_reply": "2021-03-25T09:46:59.567808Z",
          "shell.execute_reply.started": "2021-03-25T09:46:58.857706Z"
        },
        "tags": []
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.close(\"all\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-03-25T09:47:05.492038Z",
          "iopub.status.busy": "2021-03-25T09:47:05.491037Z",
          "iopub.status.idle": "2021-03-25T09:47:05.503005Z",
          "shell.execute_reply": "2021-03-25T09:47:05.502010Z",
          "shell.execute_reply.started": "2021-03-25T09:47:05.492038Z"
        },
        "tags": []
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# %matplotlib widget\n",
        "plt.plot(test_size, conf_LR_list, marker=\"o\", label=\"LR\")\n",
        "plt.plot(test_size, conf_Ridge_list, marker=\"v\", label=\"Ridge\")\n",
        "# plt.plot(test_size, conf_RidgeCV_list, marker=\"^\", label=\"RidgeCV\")\n",
        "plt.plot(test_size, conf_Lasso_list, marker=\"*\", label=\"Lasso\")\n",
        "# plt.plot(test_size, conf_SGDR_list, marker = 'x', label = \"SGDR\")\n",
        "plt.plot(test_size, conf_KNN_list, marker=\"s\", label=\"KNN\")\n",
        "plt.plot(test_size, conf_dist_KNN_list, marker=\"p\", label=\"dist_KNN\")\n",
        "\n",
        "plt.xlabel(\"Test size\")\n",
        "plt.ylabel(\"Accuracy score [%]\")\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-03-25T09:47:06.980342Z",
          "iopub.status.busy": "2021-03-25T09:47:06.980342Z",
          "iopub.status.idle": "2021-03-25T09:47:07.072097Z",
          "shell.execute_reply": "2021-03-25T09:47:07.071099Z",
          "shell.execute_reply.started": "2021-03-25T09:47:06.980342Z"
        },
        "tags": []
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "toc-autonumbering": false,
    "toc-showcode": false,
    "toc-showmarkdowntxt": false,
    "toc-showtags": false,
    "nteract": {
      "version": "0.28.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}